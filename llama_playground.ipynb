{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadityabhat/dev/llama_playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/aadityabhat/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from hf_token import hf_token\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_reordered_model_viz(model):\n",
    "    # Extract model dimensions\n",
    "    vocab_size = model.model.embed_tokens.weight.shape[0]  # 128256\n",
    "    hidden_size = model.model.embed_tokens.weight.shape[1]  # 4096\n",
    "    num_layers = len(model.model.layers)  # 32\n",
    "    num_heads = model.model.layers[0].self_attn.num_heads  # Assuming this is available\n",
    "    intermediate_size = model.model.layers[0].mlp.gate_proj.out_features  # 14336\n",
    "\n",
    "    # Define components in the desired order\n",
    "    components = [\n",
    "        ('Vocabulary\\n(Input/Output)', vocab_size),\n",
    "        ('Embedding', hidden_size),\n",
    "        ('Transformer\\nLayers', num_layers),\n",
    "        ('Hidden Size', hidden_size),\n",
    "        ('Intermediate\\nSize (MLP)', intermediate_size)\n",
    "    ]\n",
    "\n",
    "    # Colors for each component\n",
    "    colors = ['#FFA07A', '#98FB98', '#87CEFA', '#DDA0DD', '#FFD700']\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    y_pos = np.arange(len(components))\n",
    "    labels = [c[0] for c in components]\n",
    "    sizes = [c[1] for c in components]\n",
    "    bars = ax.barh(y_pos, sizes, align='center', color=colors)\n",
    "    \n",
    "    # Set y-ticks and labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels)\n",
    "    \n",
    "    # Reverse the y-axis to have the first component at the top\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add size labels on the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, f'{sizes[i]:,}', \n",
    "                ha='left', va='center', fontweight='bold')\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_title('Llama 3.1 - 8B Model Architecture', fontsize=16)\n",
    "    ax.set_xlabel('Size')\n",
    "    ax.set_xscale('log')  # Use log scale for x-axis due to large differences in sizes\n",
    "\n",
    "    # Add model details\n",
    "    details = (f\"Number of layers: {num_layers}\\n\"\n",
    "               f\"Hidden size: {hidden_size}\\n\"\n",
    "               f\"Intermediate size (MLP): {intermediate_size}\\n\"\n",
    "               f\"Number of attention heads: {num_heads}\")\n",
    "    ax.text(1.05, 0.5, details, transform=ax.transAxes, \n",
    "            verticalalignment='center', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = create_reordered_model_viz(model)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Llama 3.1 - 8B model architecture visualized here showcases a large language model with impressive capacity. At its core are 32 transformer layers, each operating on a hidden size of 4,096 dimensions. The model begins by embedding input tokens from a substantial vocabulary of 128,256 tokens into this 4,096-dimensional space. These embeddings then flow through the transformer layers, where self-attention mechanisms and feedforward neural networks process the information. Within each transformer layer, the intermediate size expands to 14,336 in the MLP (Multi-Layer Perceptron) sections, allowing for more complex computations before projecting back to the hidden size. This expansion and projection pattern is crucial for the model's expressive power. The model uses 32 attention heads, enabling it to focus on different aspects of the input simultaneously. Finally, the output is projected back to the full vocabulary size, enabling the model to predict the next token in a sequence. This architecture balances depth (32 layers) with width (4,096 hidden size), and the large vocabulary allows for nuanced token representations, contributing to the model's strong performance across various natural language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbgAAAVuCAYAAAC3O1oQAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAADqLUlEQVR4nOzdeZid8/0//ueZSCZ7Ypks1hBFBEUQe4hIosSeCEVspfalWlIqsVSqaimK8qkEjVI7JfakNKitQqv2WGoPEiKRkJzfH36Zb45ZTAgzRx6P6zrXZd7nPvf9uu/7nIPnvOb9LhSLxWIAAAAAAKDMVDR2AQAAAAAA8HUIuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsCbgBAL4lhUIhI0eObOwy+AZef/31tGzZMhMnTmzsUlhIFvbn8vjjj0/v3r0X2v4AAFgwAm4AoEm68MILUygU6gyOnnnmmYwcOTKvvPJKra8dM2bMt1vg/+/2229vciH2yJEjUygUMmXKlFqf79atW7bbbrtvtYarrroq55577rd6jO/CKaeckt69e2eTTTapHttnn33Stm3bRqxq4brmmmuy55575gc/+EEKhUK22GKLb7zPLbbYImussUatz73yyispFAr53e9+942P82266KKLMnjw4Cy//PIpFArZZ599at3uqKOOyqRJk3LLLbd8twUCAJBEwA0ANFFjx45Nt27d8sgjj+TFF1+s8fwzzzyTk08+uUkE3CeffHKtz82cOTMnnnjid1JHU/N9CLjfe++9XH755fnpT3/a2KV8qy666KLcfPPNWW655bL44os3djlNxhlnnJH77rsvPXv2zGKLLVbndl26dMkOO+zQ5AN7AIDvKwE3ANDkTJ48OQ8++GDOPvvsVFVVZezYsY1d0tfSsmXLeoMxmrY///nPWWyxxTJo0KDGLuUb+fTTTzN37tw6n7/yyiszbdq03HfffVl66aW/w8qatr///e+ZMmVKxo0bl8rKynq3HTJkSP7xj3/k5Zdf/o6qAwBgHgE3ANDkjB07Nosvvni23Xbb7LrrrjUC7jFjxmTw4MFJki233DKFQiGFQiETJkxIt27d8p///Cd///vfq8fnn3Jh6tSpOeqoo7LccsulsrIyK6+8cs4444ySAHD+KRQuueSSdO/ePZWVlVl//fXz6KOPVm+3zz775A9/+EOSVB+rUChUP1/bXL//+te/ss0226R9+/Zp27Ztttpqqzz88MM1zq9QKGTixIk55phjUlVVlTZt2mSnnXbKe++9942ubV3mzp2bc889Nz179kzLli3TuXPnHHTQQfnwww9Ltrv55puz7bbbZumll05lZWW6d++eU089NXPmzKneZosttshtt92WV199tfqadOvWLUkyYcKEFAqF/PWvf83JJ5+cZZZZJu3atcuuu+6aadOmZdasWTnqqKPSqVOntG3bNvvuu29mzZpVUsPo0aPTt2/fdOrUKZWVlVl99dVz0UUX1TineVOx3HXXXVl77bXTsmXLrL766rnhhhsadE1uuumm9O7d+2tNR/Lqq6/mkEMOyaqrrppWrVplySWXzODBg0v+4uDll19OoVDIOeecU+P1Dz74YAqFQv7yl79Uj73xxhvZb7/90rlz51RWVqZnz5657LLLSl437/peffXVOfHEE7PMMsukdevW+eijj+qsdbnllktFReP/b0FDPptJ8rvf/S4bb7xxllxyybRq1Sq9evXKddddV2N/s2bNytFHH52qqqq0a9cu22+/ff73v/81uJ4VVlih5PNcn379+iX54vMBAMB3S0sRANDkjB07NjvvvHNatGiR3XffPRdddFEeffTRrL/++kmSzTffPEcccUTOO++8/PKXv0yPHj2SJD169Mi5556bww8/PG3bts0JJ5yQJOncuXOSZMaMGenTp0/eeOONHHTQQVl++eXz4IMPZvjw4XnrrbdqTKlx1VVX5eOPP85BBx2UQqGQ3/72t9l5553z8ssvp3nz5jnooIPy5ptv5u67786VV175lef1n//8J5tttlnat2+fX/ziF2nevHn++Mc/Zosttsjf//73GvONH3744Vl88cUzYsSIvPLKKzn33HNz2GGH5ZprrmnQdfzggw9qHa+tm/eggw7KmDFjsu++++aII47I5MmTc8EFF+Rf//pXJk6cmObNmyf5Inxv27ZtjjnmmLRt2zb33XdfTjrppHz00Uc588wzkyQnnHBCpk2blv/973/V4e2XQ+JRo0alVatWOf744/Piiy/m/PPPT/PmzVNRUZEPP/wwI0eOzMMPP5wxY8ZkxRVXzEknnVT92osuuig9e/bM9ttvn8UWWyy33nprDjnkkMydOzeHHnpoyXFeeOGF7LbbbvnpT3+aYcOGZfTo0Rk8eHDuuOOObL311nVeu88++yyPPvpoDj744AZc6ZoeffTRPPjggxk6dGiWXXbZvPLKK7nooouyxRZb5Jlnnknr1q2z0korZZNNNsnYsWNz9NFHl7x+7NixadeuXXbYYYckyTvvvJMNN9wwhUIhhx12WKqqqjJu3Ljsv//++eijj3LUUUeVvP7UU09NixYtcuyxx2bWrFlp0aLF1zqPb2LOnDm1zgP/5V+aJAv22fz973+f7bffPj/+8Y8ze/bsXH311Rk8eHD+9re/Zdttt63e7oADDsif//zn7LHHHtl4441z3333lTy/MHXo0CHdu3fPxIkTa9xLAAC+ZUUAgCbkscceKyYp3n333cVisVicO3ducdllly0eeeSRJdtde+21xSTF8ePH19hHz549i3369KkxfuqppxbbtGlTfP7550vGjz/++GKzZs2Kr732WrFYLBYnT55cTFJccsklix988EH1djfffHMxSfHWW2+tHjv00EOLdf0nVZLiiBEjqn/ecccdiy1atCi+9NJL1WNvvvlmsV27dsXNN9+8emz06NHFJMV+/foV586dWz1+9NFHF5s1a1acOnVqrcebZ8SIEcUk9T623Xbb6u0feOCBYpLi2LFjS/Zzxx131BifMWNGjeMddNBBxdatWxc//fTT6rFtt922uMIKK9TYdvz48cUkxTXWWKM4e/bs6vHdd9+9WCgUittss03J9htttFGN/dRWw4ABA4orrbRSydgKK6xQTFK8/vrrq8emTZtW7Nq1a3GdddapsY/5vfjii8UkxfPPP7/Gc8OGDSu2adOm3tfXVuNDDz1UTFK84oorqsf++Mc/FpMU//vf/1aPzZ49u7jUUksVhw0bVj22//77F7t27VqcMmVKyT6HDh1a7NChQ/Xx5l3flVZaqdYavkpdn50F1adPn698D5555pnV2zf0s1ks1ry2s2fPLq6xxhrFvn37Vo89+eSTxSTFQw45pGTbPfbYo8bnsiHatGlTcj9q079//2KPHj0WaL8AAHxzjf+3iAAA8xk7dmw6d+6cLbfcMskX03zstttuufrqq0umwfg6rr322my22WZZfPHFM2XKlOpHv379MmfOnNx///0l2++2224li+5tttlmSfK15tmdM2dO7rrrruy4445ZaaWVqse7du2aPfbYI//4xz9qTCNx4IEHlkyRsNlmm2XOnDl59dVXG3TM66+/PnfffXeNx7yO9nmuvfbadOjQIVtvvXXJdenVq1fatm2b8ePHV2/bqlWr6n/++OOPM2XKlGy22WaZMWNGnn322QZfj7333ru6KzxJevfunWKxmP32269ku969e+f111/P559/XmsN06ZNy5QpU9KnT5+8/PLLmTZtWsnrl1566ey0007VP7dv3z577713/vWvf+Xtt9+us773338/Sb72oovz1/jZZ5/l/fffz8orr5yOHTvmiSeeqH5uyJAhadmyZck0PHfeeWemTJmSPffcM0lSLBZz/fXXZ9CgQSkWiyX3aMCAAZk2bVrJPpNk2LBhJTU0hm7dutX6/vvzn/9cY9sF+WzOf14ffvhhpk2bls0226zkGtx+++1JkiOOOKLkOF/udF+Y5tUOAMB3yxQlAECTMWfOnFx99dXZcsstM3ny5Orx3r1756yzzsq9996b/v37f+39v/DCC3nqqadSVVVV6/Pvvvtuyc/LL798yc/zws7aplj4Ku+9915mzJiRVVddtcZzPXr0yNy5c/P666+nZ8+eC+34m2++eZZaaqka4y1btiz5+YUXXsi0adPSqVOnWvcz/3X5z3/+kxNPPDH33XdfjUD+y+Fyfb58bh06dEjyxXzQXx6fO3dupk2bliWXXDJJMnHixIwYMSIPPfRQZsyYUaOGeftKkpVXXrnGPMqrrLJKki/mWu/SpUu9dRaLxQaf0/xmzpyZUaNGZfTo0XnjjTdK9jP/derYsWMGDRqUq666KqeeemqSL37Js8wyy6Rv375JvnjvTJ06NZdcckkuueSSWo/35ffuiiuu+LXqXpjatGlTPTf1/Oafh3yeBfls/u1vf8tpp52WJ598smR+9vnv86uvvpqKiop07969ZD+1ff4WlmKx2OA5uwEAWHgE3ABAk3HfffflrbfeytVXX52rr766xvNjx479RgH33Llzs/XWW+cXv/hFrc/PCz7nadasWa3bfd3Qc0F9V8efO3duOnXqVGMxz3nmhY5Tp05Nnz590r59+5xyyinp3r17WrZsmSeeeCLHHXdcrXN716Wuc/uqc37ppZey1VZbZbXVVsvZZ5+d5ZZbLi1atMjtt9+ec845Z4FqqM+8MP3r/DIj+WL+9NGjR+eoo47KRhttlA4dOqRQKGTo0KE1atx7771z7bXX5sEHH8yaa66ZW265JYccckj1wo/ztt9zzz0zbNiwWo+31lprlfzc2N3bC6qhn80HHngg22+/fTbffPNceOGF6dq1a5o3b57Ro0fnqquu+i5LruHDDz+s9RdKAAB8uwTcAECTMXbs2HTq1Cl/+MMfajx3ww035MYbb8zFF1+cVq1a1dspWddz3bt3z/Tp02vtKv26GtqxWVVVldatW+e5556r8dyzzz6bioqKGt3L35Xu3bvnnnvuySabbFJvMDphwoS8//77ueGGG7L55ptXj8/fbT/Pt9XJeuutt2bWrFm55ZZbSrrA559GZX4vvvhijc7a559/PskXU2jUZfnll0+rVq1qPbeGuO666zJs2LCcddZZ1WOffvpppk6dWmPbgQMHpqqqKmPHjk3v3r0zY8aM7LXXXtXPV1VVpV27dpkzZ85Cfe82JQ39bF5//fVp2bJl7rzzzlRWVlaPjx49umS7FVZYIXPnzs1LL71U0rVd2+dvYZk8eXJ++MMffmv7BwCgdubgBgCahJkzZ+aGG27Idtttl1133bXG47DDDsvHH3+cW265JckX0x8kqTUwbNOmTa3jQ4YMyUMPPZQ777yzxnNTp04tmee5oeqrY37NmjVL//79c/PNN5dM0fDOO+/kqquuyqabbpr27dsv8PEXhiFDhmTOnDnVU2TM7/PPP68+t3nd1fN3kM+ePTsXXnhhjde1adNmgaYsaajaapg2bVqNgHOeN998MzfeeGP1zx999FGuuOKKrL322vVOT9K8efOst956eeyxx752nV/utD///PNrnUd+scUWy+67756//vWvGTNmTNZcc82SjuxmzZpll112yfXXX59///vfNV7/3nvvfa0am5KGfjabNWuWQqFQch1feeWV3HTTTSWv2WabbZIk5513Xsn4ueeeu3AL//9NmzYtL730UjbeeONvZf8AANRNBzcA0CTccsst+fjjj7P99tvX+vyGG25Y3eW62267Ze21106zZs1yxhlnZNq0aamsrEzfvn3TqVOn9OrVKxdddFFOO+20rLzyyunUqVP69u2bn//857nllluy3XbbZZ999kmvXr3yySef5Omnn851112XV155ZYGnGOjVq1eSLxazGzBgQJo1a5ahQ4fWuu1pp52Wu+++O5tuumkOOeSQLLbYYvnjH/+YWbNm5be//e2CXbCFqE+fPjnooIMyatSoPPnkk+nfv3+aN2+eF154Iddee21+//vfZ9ddd83GG2+cxRdfPMOGDcsRRxyRQqGQK6+8stYpU3r16pVrrrkmxxxzTNZff/20bds2gwYN+sa19u/fPy1atMigQYNy0EEHZfr06bn00kvTqVOnvPXWWzW2X2WVVbL//vvn0UcfTefOnXPZZZflnXfeqTMQn98OO+yQE044IR999FGNXz589tlnOe2002q8ZokllsghhxyS7bbbLldeeWU6dOiQ1VdfPQ899FDuueee6qlPvmzvvffOeeedl/Hjx+eMM86o8fxvfvObjB8/Pr17985PfvKTrL766vnggw/yxBNP5J577skHH3zwledTl/vvv796Ecf33nsvn3zySfW5bb755iXd+oVCIX369MmECRO+9vFq09DP5rbbbpuzzz47AwcOzB577JF33303f/jDH7Lyyivnqaeeqt7f2muvnd133z0XXnhhpk2blo033jj33ntvXnzxxQbXdOutt2bSpElJvrjfTz31VPV12X777Ut+CXHPPfekWCxmhx12WEhXBACABisCADQBgwYNKrZs2bL4ySef1LnNPvvsU2zevHlxypQpxWKxWLz00kuLK620UrFZs2bFJMXx48cXi8Vi8e233y5uu+22xXbt2hWTFPv06VO9j48//rg4fPjw4sorr1xs0aJFcamllipuvPHGxd/97nfF2bNnF4vFYnHy5MnFJMUzzzyzRg1JiiNGjKj++fPPPy8efvjhxaqqqmKhUCjO/59XX962WCwWn3jiieKAAQOKbdu2LbZu3bq45ZZbFh988MGSbUaPHl1MUnz00UdLxsePH19ynnUZMWJEMUnxvffeq/X5FVZYobjtttvWGL/kkkuKvXr1KrZq1arYrl274pprrln8xS9+UXzzzTert5k4cWJxww03LLZq1aq49NJLF3/xi18U77zzzhp1TZ8+vbjHHnsUO3bsWExSXGGFFUrO4dprr23QOdd2LrfccktxrbXWKrZs2bLYrVu34hlnnFG87LLLikmKkydPrnGed955Z3GttdYqVlZWFldbbbUax67LO++8U1xsscWKV155Zcn4sGHDiklqfXTv3r1YLBaLH374YXHfffctLrXUUsW2bdsWBwwYUHz22WeLK6ywQnHYsGG1Hq9nz57FioqK4v/+97866zn00EOLyy23XLF58+bFLl26FLfaaqviJZdcUr1NXde3PvOucW2P+d+/H3/8cTFJcejQoV+5zz59+hR79uxZ63N1fb4a8tksFovFP/3pT8Uf/OAH1fdz9OjR1ecwv5kzZxaPOOKI4pJLLlls06ZNcdCgQcXXX3+91s9lbeq7z6NHjy7ZdrfddituuummX7lPAAAWvkKx+B2tkgQAAN+hbt26ZY011sjf/va3r72P/fffP88//3weeOCBhVhZ7dZZZ50sscQSuffee7/1Y30dt99+e7bbbrtMmjQpa665ZmOX02S8/fbbWXHFFXP11Vfr4AYAaATm4AYAgDqMGDEijz76aCZOnPitHuexxx7Lk08+mb333vtbPc43MX78+AwdOlS4/SXnnntu1lxzTeE2AEAj0cENAMD30sLo4P62/fvf/87jjz+es846K1OmTMnLL7+cli1bNnZZAABQNnRwAwBAI7nuuuuy77775rPPPstf/vIX4TYAACwgHdwAAAAAAJQlHdwAAAAAAJQlATcAAAAAAGVJwA0ALLCRI0emUCh8J8fq1q1b9tlnn+/kWE3BPvvsk27dupWMFQqFjBw58hvve8yYMSkUCnnllVe+8b6+LVtssUXWWGON781xvkuvvPJKCoVCxowZ860eZ5999knbtm0btO3CeO+W43fA9OnTc8ABB6RLly4pFAo56qijFvr9mfc9PGXKlIWyv4Yq189Obd+tAMD3g4AbgCZvXij32GOPNXYpSZIZM2Zk5MiRmTBhwjfaz8iRI2v8z3a3bt1SKBRSKBRSUVGRjh07Zs0118yBBx6Yf/7zn9/oeN+2008/PTfddNO3fpxnnnkmI0eObNIhLfV78803M3LkyDz55JONXQp8K04//fSMGTMmBx98cK688srstddejV0SC9F38YskAKDhFmvsAgCg3MyYMSMnn3xyki862Ra2tddeOz/72c+SJB9//HH++9//5tprr82ll16ao48+OmefffZCP+aCOvHEE3P88ceXjJ1++unZdddds+OOOy7UYz333HOpqPh/v5N/5plncvLJJ2eLLbbQjbeA9tprrwwdOjSVlZWNWsebb76Zk08+Od26dcvaa6/dqLV8n6ywwgqZOXNmmjdv3tilVJs5c2YWW2zR+1+O++67LxtuuGFGjBhRPeaXcgAA345F7782AaCJW2aZZbLnnnuWjJ1xxhnZY489cs455+QHP/hBDj744Eaq7guLLbbYdxZaNXYY+33SrFmzNGvWrLHL4FtSKBTSsmXLxi6jREPq+eSTT9KmTZvvoJrvzrvvvpvVV1+9sctYpHz66adp0aJFyS9EAYBFg3/7A1CW5s0B+8Ybb2THHXdM27ZtU1VVlWOPPTZz5syp3m7enKe/+93vcs4552SFFVZIq1at0qdPn/z73/8u2ecWW2xRa0f2/PN2vvLKK6mqqkqSnHzyydXTicybY/azzz7Ls88+m7feemuhnm+rVq1y5ZVXZokllsivf/3rFIvF6ufmzp2bc889Nz179kzLli3TuXPnHHTQQfnwww9L9vHYY49lwIABWWqppdKqVausuOKK2W+//aqfX5Br9eU5uAuFQj755JNcfvnl1ddk/jlz33jjjey3337p3LlzKisr07Nnz1x22WUNOvf5598dM2ZMBg8enCTZcsstq481b7qYrzrHuizItfnDH/6QlVZaKa1bt07//v3z+uuvp1gs5tRTT82yyy6bVq1aZYcddsgHH3xQcoybb7452267bZZeeulUVlame/fuOfXUU0ver9/U+eefn549e6Z169ZZfPHFs9566+Wqq66qfv7Lc3DPu4+1Pea/fw19jzXEhAkTsv766ydJ9t133+rjffnP/Z955plsueWWad26dZZZZpn89re/rbGvWbNmZcSIEVl55ZVTWVmZ5ZZbLr/4xS8ya9asBtfzVcepa97yCRMmlLz3kv83N/FTTz2VPn36pHXr1ll55ZVz3XXXJUn+/ve/p3fv3mnVqlVWXXXV3HPPPSX7nHc/nn/++ey5557p0KFDqqqq8qtf/SrFYjGvv/56dthhh7Rv3z5dunTJWWedVfL62uZ4buh3ZZK8//772WuvvdK+fft07Ngxw4YNy6RJk+qcjuHll1/OgAED0qZNmyy99NI55ZRTSr6bkppzcM87x2eeeSZ77LFHFl988Wy66aZJkmKxmNNOOy3LLrtsWrdunS233DL/+c9/ahy3LnPnzs3vf//7rLnmmmnZsmWqqqoycODAkumtRo8enb59+6ZTp06prKzM6quvnosuuqjGvrp165btttsu//jHP7LBBhukZcuWWWmllXLFFVfUW8O898XkyZNz2223Vb+/a+veHj16dAqFQv71r3/VeO70009Ps2bN8sYbb3zleU+ZMiVDhgxJ+/bts+SSS+bII4/Mp59+WuNYDTnvJBk3blz69OmTdu3apX379ll//fVLvkdqc9ddd6V169bZfffd8/nnn2fnnXfOuuuuW7LNoEGDUigUcsstt1SP/fOf/0yhUMi4ceOSJB988EGOPfbYrLnmmmnbtm3at2+fbbbZJpMmTSrZ17zrfPXVV+fEE0/MMsssk9atW+ejjz5Kktx0001ZY4010rJly6yxxhq58cYba6376quvTq9evarPdc0118zvf//7es8VAGh6dHADULbmzJmTAQMGpHfv3vnd736Xe+65J2eddVa6d+9eo8P5iiuuyMcff5xDDz00n376aX7/+9+nb9++efrpp9O5c+cGH7OqqioXXXRRDj744Oy0007ZeeedkyRrrbVWki+C3B49emTYsGELfX7Otm3bZqeddsqf/vSnPPPMM+nZs2eS5KCDDsqYMWOy77775ogjjsjkyZNzwQUX5F//+lcmTpyY5s2b5913303//v1TVVWV448/Ph07dswrr7ySG264ocZxvs61uvLKK3PAAQdkgw02yIEHHpgk6d69e5LknXfeyYYbbphCoZDDDjssVVVVGTduXPbff/989NFHOeqooxp8DTbffPMcccQROe+88/LLX/4yPXr0SJL06NFjgc5xfgvyurFjx2b27Nk5/PDD88EHH+S3v/1thgwZkr59+2bChAk57rjj8uKLL+b888/PscceWxLijxkzJm3bts0xxxyTtm3b5r777stJJ52Ujz76KGeeeWaDr0FdLr300hxxxBHZddddqwOup556Kv/85z+zxx571PqanXfeOSuvvHLJ2OOPP55zzz03nTp1qh5ryHusoXr06JFTTjklJ510Ug488MBsttlmSZKNN964epsPP/wwAwcOzM4775whQ4bkuuuuy3HHHZc111wz22yzTZIvwsztt98+//jHP3LggQemR48eefrpp3POOefk+eefb9B88A05zoL68MMPs91222Xo0KEZPHhwLrroogwdOjRjx47NUUcdlZ/+9KfZY489cuaZZ2bXXXfN66+/nnbt2pXsY7fddkuPHj3ym9/8JrfddltOO+20LLHEEvnjH/+Yvn375owzzsjYsWNz7LHHZv3118/mm29eb00N+a6cO3duBg0alEceeSQHH3xwVltttdx8880ZNmxYnfscOHBgNtxww/z2t7/NHXfckREjRuTzzz/PKaec8pXXafDgwfnBD36Q008/vToUP+mkk3LaaaflRz/6UX70ox/liSeeSP/+/TN79uyGXPrsv//+GTNmTLbZZpsccMAB+fzzz/PAAw/k4YcfznrrrZckueiii9KzZ89sv/32WWyxxXLrrbfmkEMOydy5c3PooYeW7O/FF1/Mrrvumv333z/Dhg3LZZddln322Se9evWq/v79sh49euTKK6/M0UcfnWWXXbZ6uqmqqqq89957JdvuuuuuOfTQQzN27Niss846Jc+NHTs2W2yxRZZZZpmvPO8hQ4akW7duGTVqVB5++OGcd955+fDDD0vC+Iae95gxY7LffvulZ8+eGT58eDp27Jh//etfueOOO+r8Hvnb3/6WXXfdNbvttlsuu+yyNGvWLJtttlluvvnmfPTRR2nfvn2KxWImTpyYioqKPPDAA9l+++2TJA888EAqKiqyySabJPnilyY33XRTBg8enBVXXDHvvPNO/vjHP6ZPnz555plnsvTSS5cc+9RTT02LFi1y7LHHZtasWWnRokXuuuuu7LLLLll99dUzatSovP/++9l3332z7LLLlrz27rvvzu67756tttoqZ5xxRpLkv//9byZOnJgjjzzyK687ANCEFAGgiRs9enQxSfHRRx+tHhs2bFgxSfGUU04p2XadddYp9urVq/rnyZMnF5MUW7VqVfzf//5XPf7Pf/6zmKR49NFHV4/16dOn2KdPnxrHHzZsWHGFFVao/vm9994rJimOGDGixrbzjjds2LAFP9FisbjCCisUt9122zqfP+ecc4pJijfffHOxWCwWH3jggWKS4tixY0u2u+OOO0rGb7zxxhrXsK7aG3KtRowYUfzyf0a0adOm1vPef//9i127di1OmTKlZHzo0KHFDh06FGfMmFFnTcXiF9dk/v1ee+21xSTF8ePHl2zXkHOszYJcm6qqquLUqVOrx4cPH15MUvzhD39Y/Oyzz6rHd99992KLFi2Kn376afVYbed50EEHFVu3bl2y3Zffb8Visc732/x22GGHYs+ePevdZt5nafLkybU+/9577xWXX3754pprrlmcPn16sVhs+HtsQTz66KPFJMXRo0fXeK5Pnz7FJMUrrriiemzWrFnFLl26FHfZZZfqsSuvvLJYUVFRfOCBB0pef/HFFxeTFCdOnFhvDQ09Tl3XbPz48TXeh/P2edVVV1WPPfvss8UkxYqKiuLDDz9cPX7nnXfWuAbzPlcHHnhg9djnn39eXHbZZYuFQqH4m9/8pnr8ww8/LLZq1arkszHvfTr/Phv6XXn99dcXkxTPPffc6rE5c+YU+/btW+c+Dz/88OqxuXPnFrfddttiixYtiu+99171+Jffu/POcffddy+p59133y22aNGiuO222xbnzp1bPf7LX/6yQd+p9913XzFJ8Ygjjqjx3Pz7q+1zOGDAgOJKK61UMrbCCisUkxTvv//+khorKyuLP/vZz+qtZd7rv/xdXtv92X333YtLL710cc6cOdVjTzzxRJ2fj/nNu5bbb799yfghhxxSTFKcNGlS9VhDznvq1KnFdu3aFXv37l2cOXNmybbzX8M+ffpUf9dcf/31xebNmxd/8pOflJzDvM/47bffXiwWi8WnnnqqmKQ4ePDgYu/evau323777YvrrLNO9c+ffvppyX6KxS+uW2VlZcl7eN7nb6WVVqpxbmuvvXaxa9euJd/Vd911VzFJyXfrkUceWWzfvn3x888/r3FtAIDyYooSAMraT3/605KfN9tss7z88ss1tttxxx1LOuE22GCD9O7dO7fffvtCradbt24pFosLvXt7nrZt2yb5YvHJJLn22mvToUOHbL311pkyZUr1o1evXmnbtm3Gjx+fJOnYsWOSLzrtPvvss3qPsTCvVbFYzPXXX59BgwalWCyW1DhgwIBMmzYtTzzxxALvtzYLco5f93WDBw9Ohw4dqn/u3bt3kmTPPfcsmZO8d+/emT17dsn0Aq1atar+548//jhTpkzJZpttlhkzZuTZZ59tcL31ncf//ve/PProo1/r9XPmzMnuu++ejz/+ODfeeGP1nMgNfY8tTG3bti2Zh75FixbZYIMNSj7b1157bXr06JHVVlutpK6+ffsmSYPqashxvk7tQ4cOrf551VVXTceOHdOjR4/q90vy/947tR3rgAMOqP7nZs2aZb311kuxWMz+++9fPd6xY8esuuqqDa71q74r77jjjjRv3jw/+clPqscqKipqdDXP77DDDqv+53l/oTF79uwaU680pJ577rmn+q8j5p/+qKF/4XH99denUCiULOo4f23zzP85nDZtWqZMmZI+ffrk5ZdfzrRp00pet/rqq1f/hUHyRRf2glzzhth7773z5ptvlrxfx44dm1atWmWXXXZp0D6+fI8OP/zwJCn5zm7Ied999935+OOPc/zxx9eYO33+azjPX/7yl+y222456KCD8sc//rFk7ut11lknbdu2zf3335/ki07tZZddNnvvvXeeeOKJzJgxI8ViMf/4xz9KrnFlZWX1fubMmZP3338/bdu2zaqrrlrrvyuGDRtWcm5vvfVWnnzyyQwbNqzku3rrrbeuMSd6x44d88knn+Tuu++usV8AoLwIuAEoW/PmWJ3f4osvXuu8wD/4wQ9qjK2yyiq1zovalE2fPj1Jqqc0eOGFFzJt2rR06tQpVVVVJY/p06fn3XffTZL06dMnu+yyS04++eQstdRS2WGHHTJ69Oha5ypemNfqvffey9SpU3PJJZfUqG/fffdNkuoav6kFOcev+7rll1++5Od5Acpyyy1X6/j878X//Oc/2WmnndKhQ4e0b98+VVVV1eHql4O1r+O4445L27Zts8EGG+QHP/hBDj300EycOLHBrz/xxBNz33335aqrrqqeXiZp+HtsYVp22WVrBGpf/my/8MIL+c9//lOjplVWWSVJw95XDTnOwqi9Q4cODXqPzFPb+6xly5ZZaqmlaow3pNaGfFe++uqr6dq1a1q3bl2y3ZensJmnoqIiK620UsnYvGvfkO+KFVdcseTnV199NUnN75+qqqosvvjiX7m/l156KUsvvXSWWGKJerebOHFi+vXrlzZt2qRjx46pqqrKL3/5yyQ1P4dfvg/JN39/fNnWW2+drl27ZuzYsUm+mCrmL3/5S3bYYYcaU9fU5cvXrHv37qmoqCi5Dw0575deeilJssYaa3zlMSdPnpw999wzu+yyS84///wa7/lmzZplo402ygMPPJDki4B7s802y6abbpo5c+bk4YcfzjPPPJMPPvigJOCeO3du9WLKlZWVWWqppVJVVZWnnnqq1u/Jhr6Pki9+2TS/Qw45JKusskq22WabLLvsstlvv/1yxx13fOW5AwBNjzm4AShbzZo1W6j7KxQKNRZIS7JQFwH8puYt9jgvdJo7d246depUHY582bxQq1Ao5LrrrsvDDz+cW2+9NXfeeWf222+/nHXWWXn44YerO8MXtrlz5yb5osO5rrl8581f/k193XNckNfV9Z6ra3ze+2nq1Knp06dP2rdvn1NOOSXdu3dPy5Yt88QTT+S4446rvk7fRI8ePfLcc8/lb3/7W+64445cf/31ufDCC3PSSSfl5JNPrve1N910U84444yceuqpGThwYMlzDX2PLUxfdT3n1bXmmmvm7LPPrnXbLwfKX/c4tXWuJnV/L3zd98hXbbsgr2/oaxvT/F2335WXXnopW221VVZbbbWcffbZWW655dKiRYvcfvvtOeecc2p8Dr/JNW+oZs2aZY899sill16aCy+8MBMnTsybb75Z8pcFC+rL79kFPe+G6Nq1a7p27Zrbb789jz32WPUc5/PbdNNN8+tf/zqffvppHnjggZxwwgnp2LFj1lhjjTzwwAPVazrMH3Cffvrp+dWvfpX99tsvp556apZYYolUVFTkqKOOqrXOb/I+6tSpU5588snceeedGTduXMaNG5fRo0dn7733zuWXX/619wsAfPcE3AAsEl544YUaY88//3y6detW/fPiiy9e65+ez+sIm6euwOvbNn369Nx4441ZbrnlqhdX7N69e+65555ssskmDfof/Q033DAbbrhhfv3rX+eqq67Kj3/841x99dUlUyI05FrVprbrUlVVlXbt2mXOnDnp16/fV9bXEF91/RtyjgvzdQ0xYcKEvP/++7nhhhtKFgScPHnyN973/Nq0aZPddtstu+22W2bPnp2dd945v/71rzN8+PAaUw7M8/zzz2fYsGHZcccdqzs657eg77GGWBifoe7du2fSpEnZaqutvtXP5Lzu4alTp5aMf/l7odytsMIKGT9+fGbMmFHSxf3iiy/Wuv3cuXPz8ssvV3dtJ1+8l5J85XdFXcdPvvj+mb8z/L333mtQx3T37t1z55135oMPPqizi/vWW2/NrFmzcsstt5R0Z38b0+wsiL333jtnnXVWbr311owbNy5VVVUZMGBAg1//wgsvlHQyv/jii5k7d271fWjoec/7y41///vfdXbuz9OyZcv87W9/S9++fTNw4MD8/e9/r7Hw5mabbZbZs2fnL3/5S954443qIHvzzTevDrhXWWWVksWLr7vuumy55Zb505/+VLKvqVOn1vgLhtrM/z76sueee67GWIsWLTJo0KAMGjQoc+fOzSGHHJI//vGP+dWvfvWV1wAAaDpMUQLAIuGmm24qmQ/5kUceyT//+c9ss8021WPdu3fPs88+m/fee696bNKkSTWmeZgX/nw58EqSzz77LM8++2zeeuuthVr/zJkzs9dee+WDDz7ICSecUB3oDRkyJHPmzMmpp55a4zWff/55dY0ffvhhja7DtddeO0lqTMXRkGtVmzZt2tS4Js2aNcsuu+yS66+/vrr7fH7zX+uGmjc39JePtSDnuDBetyDmdYLOf5zZs2fnwgsvXCj7T5L333+/5OcWLVpk9dVXT7FYrHNu8enTp2ennXbKMsssk8svv7zWoLih77EFUdc9XBBDhgzJG2+8kUsvvbTGczNnzswnn3zytfc9v3mh37y5hJMvurcvueSShbL/pmLAgAH57LPPSq7n3Llz84c//KHO11xwwQXV/1wsFnPBBRekefPm2WqrrRb4+P369Uvz5s1z/vnnl3xOzj333Aa9fpdddkmxWKz1rxXm7a+2z+G0adMyevToBa53YVprrbWy1lpr5f/+7/9y/fXXZ+jQoSVz+n+VL9+j888/P0mqv7Mbet79+/dPu3btMmrUqHz66aclz9XWtd6hQ4fceeed6dSpU7beeuvqKU7m6d27d5o3b54zzjgjSyyxRHUAvtlmm+Xhhx/O3//+95Lu7Xm1fvlY1157bcm/k+rTtWvXrL322rn88stLpjS5++6788wzz5Rs++XvzIqKiuq/KJr33f9t/TsdAFi4dHADsEhYeeWVs+mmm+bggw/OrFmzcu6552bJJZfML37xi+pt9ttvv5x99tkZMGBA9t9//7z77ru5+OKL07Nnz3z00UfV27Vq1Sqrr756rrnmmqyyyipZYoklssYaa2SNNdbIG2+8kR49emTYsGFfe6HJN954I3/+85+TfBFAPvPMM7n22mvz9ttv52c/+1kOOuig6m379OmTgw46KKNGjcqTTz6Z/v37p3nz5nnhhRdy7bXX5ve//3123XXXXH755bnwwguz0047pXv37vn4449z6aWXpn379vnRj360wNeqNr169co999yTs88+O0svvXRWXHHF9O7dO7/5zW8yfvz49O7dOz/5yU+y+uqr54MPPsgTTzyRe+65Jx988MECXZ+11147zZo1yxlnnJFp06alsrIyffv2zVVXXdXgc5zfglybr2vjjTfO4osvnmHDhuWII45IoVDIlVdeuVCnOujfv3+6dOmSTTbZJJ07d85///vfXHDBBdl2223rnMv35JNPzjPPPJMTTzwxN998c8lz3bt3z0YbbdTg91iSjBkzJvvuu29Gjx6dffbZp85au3fvno4dO+biiy9Ou3bt0qZNm/Tu3bvGfLr12WuvvfLXv/41P/3pTzN+/PhssskmmTNnTp599tn89a9/zZ133lnrtAkLqmfPntlwww0zfPjw6u7gq6++Op9//vk33ndTsuOOO2aDDTbIz372s7z44otZbbXVcsstt1R/Pr/8y4+WLVvmjjvuyLBhw9K7d++MGzcut912W375y19+rWlrqqqqcuyxx2bUqFHZbrvt8qMf/Sj/+te/Mm7cuAZ17m655ZbZa6+9ct555+WFF17IwIEDM3fu3DzwwAPZcsstc9hhh6V///7VHbsHHXRQpk+fnksvvTSdOnVq9ABz7733zrHHHpskCzw9yeTJk7P99ttn4MCBeeihh/LnP/85e+yxR374wx8mSYPPu3379jnnnHNywAEHZP31188ee+yRxRdfPJMmTcqMGTNqnbZjqaWWyt13351NN900/fr1yz/+8Y/qRYpbt26dXr165eGHH86gQYOq30Obb755Pvnkk3zyySc1Au7tttsup5xySvbdd99svPHGefrppzN27Nga873XZ9SoUdl2222z6aabZr/99ssHH3yQ888/Pz179qxexyL5YjHXDz74IH379s2yyy6bV199Neeff37WXnvt6r+SWhj/TgcAvn0CbgAWCXvvvXcqKipy7rnn5t13380GG2yQCy64IF27dq3epkePHrniiity0kkn5Zhjjsnqq6+eK6+8MldddVUmTJhQsr//+7//y+GHH56jjz46s2fPzogRIxq0MFdDPPnkk9lrr71SKBTSrl27LLfcchk0aFAOOOCAbLDBBjW2v/jii9OrV6/88Y9/zC9/+csstthi6datW/bcc89ssskmSb4Iwh955JFcffXVeeedd9KhQ4dssMEGGTt2bI1QsSHXqjZnn312DjzwwJx44omZOXNmdfDVuXPnPPLIIznllFNyww035MILL8ySSy6Znj175owzzljg69OlS5dcfPHFGTVqVPbff//MmTMn48ePX6BznN/Xfd2CWHLJJfO3v/0tP/vZz3LiiSdm8cUXz5577pmtttpqgaYiqM9BBx2UsWPH5uyzz8706dOz7LLL5ogjjsiJJ55Y52vmddCfdtppNZ4bNmxYNtpooyQNe48l/28R1K96rzRv3jyXX355hg8fnp/+9Kf5/PPPM3r06AW63hUVFbnppptyzjnn5IorrsiNN96Y1q1bZ6WVVsqRRx5ZMnXGNzV27NgcdNBB+c1vfpOOHTtm//33z5Zbbpmtt956oR2jsTVr1iy33XZbjjzyyFx++eWpqKjITjvtlBEjRmSTTTapMcVNs2bNcscdd+Tggw/Oz3/+87Rr1y4jRozISSed9LVrOO2009KyZctcfPHF1b8Uu+uuu7Lttts26PWjR4/OWmutlT/96U/5+c9/ng4dOmS99dbLxhtvnOSLRQavu+66nHjiiTn22GPTpUuXHHzwwamqqsp+++33teteGH784x/nuOOOS/fu3Wv9nq/PNddck5NOOinHH398FltssRx22GE588wzq59fkPPef//906lTp/zmN7/JqaeemubNm2e11VbL0UcfXefxl1lmmdxzzz3ZbLPNsvXWW+f++++v/qXEvG7tTTfdtHr7Ll26ZOWVV86LL75YI+D+5S9/mU8++SRXXXVVrrnmmqy77rq57bbbcvzxxzf4egwcODDXXnttTjzxxAwfPjzdu3fP6NGjc/PNN5f8u3zPPffMJZdckgsvvDBTp05Nly5dsttuu2XkyJGpqPCHzgBQTgrFhdk6BABNzCuvvJIVV1wxZ555ZnV3HLVzrfimhgwZkldeeSWPPPJIY5fCQnLTTTdlp512yj/+8Y+SX2awcE2ZMiVdu3bNSSedlF/96leNXQ4AQFnRwQ0AwDdWLBYzYcKE6ul1KD8zZ84sWUh0zpw5Of/889O+ffusu+66jVjZ99+YMWMyZ86c7LXXXo1dCgBA2RFwAwDwjRUKhbz77ruNXQbfwOGHH56ZM2dmo402yqxZs3LDDTfkwQcfzOmnn14SfLPw3HfffXnmmWfy61//OjvuuGO6devW2CUBAJQdATcAAJC+ffvmrLPOyt/+9rd8+umnWXnllXP++efnsMMOa+zSvrdOOeWUPPjgg9lkk01y/vnnN3Y5AABlyRzcAAAAAAB8I/fff3/OPPPMPP7443nrrbdy4403Zscdd6z3NRMmTMgxxxyT//znP1luueVy4oknZp999lmg41oeGgAAAACAb+STTz7JD3/4w/zhD39o0PaTJ0/Otttumy233DJPPvlkjjrqqBxwwAG58847F+i4OrgBAAAAAFhoCoXCV3ZwH3fccbntttvy73//u3ps6NChmTp1au64444GH0sHNwAAAAAAJWbNmpWPPvqo5DFr1qyFtv+HHnoo/fr1KxkbMGBAHnrooQXaj0Umv6dOLqza2CUsEkYUr2rsEgAAAAC+oV6NXUCTtyhmbcURu+fkk08uGRsxYkRGjhy5UPb/9ttvp3PnziVjnTt3zkcffZSZM2emVatWDdqPgBsAAAAAgBLDhw/PMcccUzJWWVnZSNXUTcANAAAAAECJysrKbzXQ7tKlS955552SsXfeeSft27dvcPd2Yg5uAAAAAAC+YxtttFHuvffekrG77747G2200QLtR8ANAAAAAMA3Mn369Dz55JN58sknkySTJ0/Ok08+mddeey3JF1Oe7L333tXb//SnP83LL7+cX/ziF3n22Wdz4YUX5q9//WuOPvroBTquKUoAAAAAAOqhS/irPfbYY9lyyy2rf543f/ewYcMyZsyYvPXWW9Vhd5KsuOKKue2223L00Ufn97//fZZddtn83//9XwYMGLBAxy0Ui8XiwjkFmpJFcWXXxjCieFVjlwAAAADwDfVq7AKavFMXwaztV8XnGruEBvHLBwAAAAAAypKAGwAAAACAsiTgBgAAAACgLFlkEgAAAACgHrqEmy73BgAAAACAsiTgBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkkUmAQAAAADqoUu46XJvAAAAAAAoSwJuAAAAAADKkoAbAAAAAICyZA5uAAAAAIB66BJuutwbAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsCbgBAAAAAChLFpkEAAAAAKiHLuGmy70BAAAAAKAsCbgBAAAAAChLAm4AAAAAAMqSgBsAAAAAgLJkkUkAAAAAgHoUGrsA6qSDGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkkUmAQAAAADqoUu46XJvAAAAAAAoSwJuAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsWWQSAAAAAKAeuoSbLvcGAAAAAICyJOAGAAAAAKAsCbgBAAAAAChL5uD+ln344Ydp3rx52rZtW+92r732WpZffvnvqCoAAAAAoKF0CTdd7s234PPPP89tt92WwYMHp2vXrnnppZcye/bsHHbYYenatWtatmyZFVZYIaNGjap+zbBhw7LGGmvkzDPPzFtvvdWI1QMAAAAAlAcB90L09NNP52c/+1mWXXbZ7L333qmqqsr48ePzwx/+MOedd15uueWW/PWvf81zzz2XsWPHplu3btWv/etf/5oDDzww11xzTZZbbrn86Ec/yjXXXJNPP/208U4IAAAAAKAJE3B/Q++//35+//vfZ9111816662Xl19+ORdeeGHeeuutXHjhhdloo42SfDEFyQ9+8INsuummWWGFFbLppptm9913r95PVVVVjjjiiDz22GN5+umns9Zaa+XYY49N165d89Of/jQPP/xwY50iAAAAAECTVCgWi8XGLqKcjRw5MieffHI222yzjB07Nsstt1yt2z3xxBPZeuuts+SSS2bgwIHZbrvt0r9//3r3PXfu3Jx55pn51a9+ldatW2fq1Km1bjdr1qzMmjWrZOzMDr2ymN9ffOtGFK9q7BIAAAAAvqFejV1Ak3deYdXGLuE7d0TxucYuoUEkoN/QgQcemFNPPTVvv/12evbsmX333Tf33Xdf5s6dW7Lduuuum8mTJ+fUU0/NzJkzM2TIkOy666617vP111/Pb37zm6y55po5+eSTM3jw4Fx33XV11jBq1Kh06NCh5PFAPlio5wkAAAAAi6qKRfBRLnRwL0QPPvhgLr/88lxzzTVp165dfvzjH2evvfZKz549a2x75513ZuDAgXn//fezxBJL5OOPP87111+fK664In//+9+z8cYbZ5999sngwYPTvn37eo+rg7vx6OAGAAAAyp8O7q9ywSLYwX1YmXRwL9bYBXyfbLzxxtl4443z+9//PjfddFPGjBmT3/3ud/nXv/6Vu+++O127ds0666yTioqKXHvttenSpUs6duyYJNlxxx3z8ssvZ6+99sqll16a7t27N/i4lZWVqaysLBkTbgMAAAAA33cC7m9By5YtM3To0AwdOjRvvvlm2rZtm3bt2uW3v/1tXnjhhTRr1izrr79+br/99lRUfBFEX3jhhVlllVVSKBQauXoAAAAAgPJgipLvqZMXwT+baAymKAEAAADKnylKvoopSpouHdwAAAAAAPUwGXDT5d4AAAAAAFCWBNwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFmyyCQAAAAAQD10CTdd7g0AAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTAIAAAAA1EOXcNPl3gAAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZckc3AAAAAAA9dAl3HS5NwAAAAAAlCUBNwAAAAAAZUnADQAAAABAWRJwAwAAAABQliwyCQAAAABQD13CTZd7AwAAAABAWRJwAwAAAABQlgTcAAAAAACUJQE3AAAAAABlySKTAAAAAAD10CXcdLk3AAAAAACUJQE3AAAAAABlScANAAAAAEBZEnADAAAAAFCWLDIJAAAAAFAPXcJNl3sDAAAAAEBZEnADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGXJIpMAAAAAAPUoNHYB1EkHNwAAAAAAZUnADQAAAABAWRJwAwAAAABQlgTcAAAAAACUJYtMAgAAAADUQ5dw0+XeAAAAAABQlgTcAAAAAACUJVOUfE+t0dgFLCo+fqWxK1g0tOvW2BUAAAAA0AQJuAEAAAAA6mEajKbLvQEAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAeugSbrrcGwAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACohy7hpsu9AQAAAACgLAm4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyZJFJAAAAAIB66BJuutwbAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsCbgBAAAAAChLFpkEAAAAAKiHLuGmy70BAAAAAKAsCbgBAAAAAChLAm4AAAAAAMqSObgBAAAAAOqhS7jpcm8AAAAAAChLAm4AAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCxZZBIAAAAAoB66hJsu9wYAAAAAgLIk4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypJFJgEAAAAA6qFLuOlybwAAAAAAKEsCbgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4v0MjR47M2muv3dhlAAAAAAALoGIRfJSLcqr1a9tnn31SKBRSKBTSvHnzdO7cOVtvvXUuu+yyzJ079zur49hjj829995bUteOO+74nR0fAAAAAOD7ZJEIuJNk4MCBeeutt/LKK69k3Lhx2XLLLXPkkUdmu+22y+eff/6d1NC2bdssueSS38mxAAAAAAC+7xaZgLuysjJdunTJMsssk3XXXTe//OUvc/PNN2fcuHEZM2ZMkmTq1Kk54IADUlVVlfbt26dv376ZNGlS9T4mTZqULbfcMu3atUv79u3Tq1evPPbYY0mSMWPGpGPHjrnpppvygx/8IC1btsyAAQPy+uuvV79+/ilKRo4cmcsvvzw333xzdXf5hAkTkiSvv/56hgwZko4dO2aJJZbIDjvskFdeeeW7uEwAAAAAAGVjkQm4a9O3b9/88Ic/zA033JAkGTx4cN59992MGzcujz/+eNZdd91stdVW+eCDD5IkP/7xj7Psssvm0UcfzeOPP57jjz8+zZs3r97fjBkz8utf/zpXXHFFJk6cmKlTp2bo0KG1HvvYY4/NkCFDqjvL33rrrWy88cb57LPPMmDAgLRr1y4PPPBAJk6cmLZt22bgwIGZPXv2t39RAAAAAADKxGKNXUBjW2211fLUU0/lH//4Rx555JG8++67qaysTJL87ne/y0033ZTrrrsuBx54YF577bX8/Oc/z2qrrZYk+cEPflCyr88++ywXXHBBevfunSS5/PLL06NHjzzyyCPZYIMNSrZt27ZtWrVqlVmzZqVLly7V43/+858zd+7c/N///V8KhUKSZPTo0enYsWMmTJiQ/v37f2vXAgAAAACoqdDYBVCnRT7gLhaLKRQKmTRpUqZPn15jjuyZM2fmpZdeSpIcc8wxOeCAA3LllVemX79+GTx4cLp371697WKLLZb111+/+ufVVlstHTt2zH//+98aAXddJk2alBdffDHt2rUrGf/000+r6/iyWbNmZdasWSVjn2Vumi/aDfoAAAAAwPfcIh9w//e//82KK66Y6dOnp2vXrtXzYM+vY8eOSb6YN3uPPfbIbbfdlnHjxmXEiBG5+uqrs9NOOy20eqZPn55evXpl7NixNZ6rqqqq9TWjRo3KySefXDI2OEtktyy10OoCAAAAAGhqFumA+7777svTTz+do48+Ossuu2zefvvtLLbYYunWrVudr1lllVWyyiqr5Oijj87uu++e0aNHVwfcn3/+eR577LHqbu3nnnsuU6dOTY8ePWrdV4sWLTJnzpySsXXXXTfXXHNNOnXqlPbt2zfoPIYPH55jjjmmZOz2Dr0a9FoAAAAAgHK1yMxhMWvWrLz99tt544038sQTT+T000/PDjvskO222y577713+vXrl4022ig77rhj7rrrrrzyyit58MEHc8IJJ+Sxxx7LzJkzc9hhh2XChAl59dVXM3HixDz66KMl4XXz5s1z+OGH55///Gcef/zx7LPPPtlwww3rnJ6kW7dueeqpp/Lcc89lypQp+eyzz/LjH/84Sy21VHbYYYc88MADmTx5ciZMmJAjjjgi//vf/2rdT2VlZdq3b1/yMD0JAAAAACwcFYvgo1yUU63fyB133JGuXbumW7duGThwYMaPH5/zzjsvN998c5o1a5ZCoZDbb789m2++efbdd9+sssoqGTp0aF599dV07tw5zZo1y/vvv5+99947q6yySoYMGZJtttmmZGqQ1q1b57jjjssee+yRTTbZJG3bts0111xTZ00/+clPsuqqq2a99dZLVVVVJk6cmNatW+f+++/P8ssvn5133jk9evTI/vvvn08//bTBHd0AAAAAAIuCQrFYLDZ2Ed8HY8aMyVFHHZWpU6c2dilJkusLqzZ2CYuEXT46vbFLWDS069bYFQAAAMD3mKluv8pdi2DW1r/4XGOX0CCLTAc3AAAAAADfLwJuAAAAAADKkoB7Idlnn32azPQkAAAAAMDC09gLPlpksm7lVCsAAAAAAFQTcAMAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZWmxxi4AAAAAAKAp0yXcdLk3AAAAAACUJQE3AAAAAABlScANAAAAAEBZEnADAAAAAFCWLDIJAAAAAFCPQqGxK6AuOrgBAAAAAChLAm4AAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCxZZBIAAAAAoB4VhWJjl0AddHADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFmyyCQAAAAAQD0KhcaugLro4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypI5uAEAAAAA6mEK7qZLBzcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTAIAAAAA1KNQKDZ2CdRBBzcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTAIAAAAA1KNQaOwKqIsObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACoh0Ummy4d3AAAAAAAlCUd3N9TG68ws7FLWCQU336ssUtYJBRad23sEhYNzSobuwIAAACABaKDGwAAAACAsiTgBgAAAACgLJmiBAAAAACgHhWFYmOXQB10cAMAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZckc3AAAAAAA9Sg0dgHUSQc3AAAAAABlScANAAAAAEBZEnADAAAAAFCWBNwAAAAAAJQli0wCAAAAANSjYJXJJksHNwAAAAAAZUnADQAAAABAWRJwAwAAAABQlgTcAAAAAACUJYtMAgAAAADUwyKTTZcObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACoR6FQbOwSqIMObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACoR0WhsSugLjq4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyZA5uAAAAAIB6FMzB3WTp4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAehRSbOwSqIMObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4G8GYMWPSsWPHxi4DAAAAAKCsCbgbwW677Zbnn3+++ueRI0dm7bXXbryCAAAAAIA6FQqL3qNcLNbYBSyKWrVqlVatWjV2GQAAAAAAZU0HdyOYf4qSMWPG5OSTT86kSZNSKBRSKBQyZsyYFIvFjBw5Mssvv3wqKyuz9NJL54gjjmjcwgEAAAAAmhAd3I1st912y7///e/ccccdueeee5IkHTp0yPXXX59zzjknV199dXr27Jm33347kyZNauRqAQAAAACaDgF3I2vVqlXatm2bxRZbLF26dKkef+2119KlS5f069cvzZs3z/LLL58NNtigESsFAAAAAGhaTFHSRA0ePDgzZ87MSiutlJ/85Ce58cYb8/nnn9e67axZs/LRRx+VPGYVi99xxQAAAADw/dTYCz5aZLJuAu4marnllstzzz2XCy+8MK1atcohhxySzTffPJ999lmNbUeNGpUOHTqUPM6f9lEjVA0AAAAA8N0RcDcBLVq0yJw5c2qMt2rVKoMGDcp5552XCRMm5KGHHsrTTz9dY7vhw4dn2rRpJY/DO7T/LkoHAAAAAGg05uBuArp165bJkyfnySefzLLLLpt27drlL3/5S+bMmZPevXundevW+fOf/5xWrVplhRVWqPH6ysrKVFZWlox9Uk5/RwAAAAAA8DXo4G4CdtlllwwcODBbbrllqqqq8pe//CUdO3bMpZdemk022SRrrbVW7rnnntx6661ZcsklG7tcAAAAAIAmoVAsWo3w++itbss3dgmLhC53/7ixS1gkFFYa1NglLBqaVX71NgAAAHwP9WrsApq8f3dcsbFL+M6tMXVyY5fQIDq4AQAAAAD4xv7whz+kW7duadmyZXr37p1HHnmk3u3PPffcrLrqqmnVqlWWW265HH300fn0008X6JgCbgAAAAAAvpFrrrkmxxxzTEaMGJEnnngiP/zhDzNgwIC8++67tW5/1VVX5fjjj8+IESPy3//+N3/6059yzTXX5Je//OUCHVfADQAAAADAN3L22WfnJz/5Sfbdd9+svvrqufjii9O6detcdtlltW7/4IMPZpNNNskee+yRbt26pX///tl9992/suv7ywTcAAAAAACUmDVrVj766KOSx6xZs2rddvbs2Xn88cfTr1+/6rGKior069cvDz30UK2v2XjjjfP4449XB9ovv/xybr/99vzoRz9aoDoF3AAAAAAA9SgUFr3HqFGj0qFDh5LHqFGjar0+U6ZMyZw5c9K5c+eS8c6dO+ftt9+u9TV77LFHTjnllGy66aZp3rx5unfvni222MIUJQAAAAAAfDPDhw/PtGnTSh7Dhw9faPufMGFCTj/99Fx44YV54okncsMNN+S2227LqaeeukD7WWyhVQQAAAAAwPdCZWVlKisrG7TtUkstlWbNmuWdd94pGX/nnXfSpUuXWl/zq1/9KnvttVcOOOCAJMmaa66ZTz75JAceeGBOOOGEVFQ0rDdbBzcAAAAAAF9bixYt0qtXr9x7773VY3Pnzs29996bjTbaqNbXzJgxo0aI3axZsyRJsVhs8LF1cAMAAAAA1KPQ2AWUgWOOOSbDhg3Leuutlw022CDnnntuPvnkk+y7775Jkr333jvLLLNM9TzegwYNytlnn5111lknvXv3zosvvphf/epXGTRoUHXQ3RACbgAAAAAAvpHddtst7733Xk466aS8/fbbWXvttXPHHXdULzz52muvlXRsn3jiiSkUCjnxxBPzxhtvpKqqKoMGDcqvf/3rBTpuobgg/d6Ujbe6Ld/YJSwSutz948YuYZFQWGlQY5ewaGjWsHm1AAAA+L7p1dgFNHnPLL5iY5fwnVv9w8mNXUKDmIMbAAAAAICyJOAGAAAAAKAsmYMbAAAAAKAehYJZnpsqHdwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYsMgkAAAAAUI9CobEroC46uAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsiTgBgAAAACgLFlkEgAAAACgHhUWmWyydHADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFmyyCQAAAAAQD0KhWJjl0AddHADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGXJHNwAAAAAAPUoNHYB1EkHNwAAAAAAZUnADQAAAABAWRJwAwAAAABQlgTcAAAAAACUJYtMAgAAAADUo2CVySZLBzcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTH5PdRm1emOXsEgo3vBAY5ewSCgcsXljl7BoaLlkY1ewaCg0a+wKAAAAWECFQrGxS6AOOrgBAAAAAChLAm4AAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCxZZBIAAAAAoB4VhcaugLro4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAehQsMtlk6eAGAAAAAKAsCbgBAAAAAChLAm4AAAAAAMqSObgBAAAAAOphDu6mSwc3AAAAAABlScANAAAAAEBZEnADAAAAAFCWBNwAAAAAAJQli0wCAAAAANSjkGJjl0AddHADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFmyyCQAAAAAQD0KhcaugLro4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAehQqrDLZVOngBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyZJFJAAAAAIB6FLQJN1luDQAAAAAAZUnADQAAAABAWRJwAwAAAABQlgTcAAAAAACUJYtMAgAAAADUo1Bo7Aqoiw5uAAAAAADKkoC7CXrllVdSKBTy5JNPJkkmTJiQQqGQqVOnNmpdAAAAAABNiYAbAAAAAICyZA5uAAAAAID6VJiEu6nSwf0VXn311QwaNCiLL7542rRpk549e+b2229P8v+mDrnzzjuzzjrrpFWrVunbt2/efffdjBs3Lj169Ej79u2zxx57ZMaMGdX7vOOOO7LpppumY8eOWXLJJbPddtvlpZdeaqxTBAAAAAAoSzq4v8Khhx6a2bNn5/7770+bNm3yzDPPpG3btiXbjBw5MhdccEFat26dIUOGZMiQIamsrMxVV12V6dOnZ6eddsr555+f4447LknyySef5Jhjjslaa62V6dOn56STTspOO+2UJ598MhUVfucAAAAAANAQAu6v8Nprr2WXXXbJmmuumSRZaaWVamxz2mmnZZNNNkmS7L///hk+fHheeuml6m133XXXjB8/vjrg3mWXXUpef9lll6WqqirPPPNM1lhjjW/zdAAAAAAAvje0C3+FI444ojrAHjFiRJ566qka26y11lrV/9y5c+e0bt26JAjv3Llz3n333eqfX3jhhey+++5ZaaWV0r59+3Tr1i3JF2H61zFr1qx89NFHJY9Zn839WvsCAAAAACgXAu6vcMABB+Tll1/OXnvtlaeffjrrrbdezj///JJtmjdvXv3PhUKh5Od5Y3Pn/r/AedCgQfnggw9y6aWX5p///Gf++c9/Jklmz579tWocNWpUOnToUPIYdfPLX2tfAAAAAECpQsWi9ygXZVRq41luueXy05/+NDfccEN+9rOf5dJLL/3a+3r//ffz3HPP5cQTT8xWW22VHj165MMPP/xG9Q0fPjzTpk0reQzfoeZUKgAAAAAA3yfm4P4KRx11VLbZZpusssoq+fDDDzN+/Pj06NHja+9v8cUXz5JLLplLLrkkXbt2zWuvvZbjjz/+G9VYWVmZysrKkrFic7+7AAAAAAC+36SgX2HOnDk59NBD06NHjwwcODCrrLJKLrzwwq+9v4qKilx99dV5/PHHs8Yaa+Too4/OmWeeuRArBgAAAABYNBSKxWKxsYtg4Sv+ZWBjl7BIKL42vbFLWCRUHHFCY5ewaGi5ZGNXsGgoNGvsCgAAAL6kV2MX0OS9vfIKjV3Cd67Li682dgkNYooSAAAAAIB6FAqFxi6BOpiiBAAAAACAsiTgBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkkUmAQAAAADqUdAm3GS5NQAAAAAAlCUBNwAAAAAAZUnADQAAAABAWRJwAwAAAABQliwyCQAAAABQn0KhsSugDjq4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyZA5uAAAAAIB6FLQJN1luDQAAAAAAZUnADQAAAABAWRJwAwAAAABQlgTcAAAAAACUJYtMAgAAAADUo1BRaOwSqIMObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACoR8Eak02WDm4AAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCwJuAEAAAAAKEsWmQQAAAAAqEdBm3CT5dYAAAAAAFCWBNwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFmyyCQAAAAAQH0qCo1dAXXQwQ0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCVzcAMAAAAA1KNgCu4mSwc3AAAAAABlScANAAAAAEBZEnADAAAAAFCWzMH9fdWyWWNXsEgorLt0Y5ewSCi+/5/GLmGRUFhqjcYuYdFQuURjV7BoKPj3IAAAwKJAwA0AAAAAUI9ChVUmmypTlAAAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZUnADQAAAABAWbLIJAAAAABAPQrahJsstwYAAAAAgLIk4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypJFJgEAAAAA6lEoFBq7BOqggxsAAAAAgLIk4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypJFJgEAAAAA6qNNuMlyawAAAAAAKEsCbgAAAAAAypKAGwAAAACAsmQObgAAAACAehQKjV0BddHBDQAAAABAWRJwAwAAAABQlgTcAAAAAACUJQE3AAAAAABlySKTAAAAAAD1KFRYZbKp0sENAAAAAEBZEnADAAAAAFCWBNwAAAAAAJQlATcAAAAAAGXJIpMAAAAAAPUoaBNustwaAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsCbgBAAAAAChLFpkEAAAAAKhPodDYFVAHHdwAAAAAAJQlATcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYsMgkAAAAAUI+CNuEmy635DkyYMCGFQiFTp05t7FIAAAAAAL43BNzfgY033jhvvfVWOnTo0NilAAAAAAB8b5ii5DvQokWLdOnSpbHLAAAAAAD4XtHBvZC8+uqrGTRoUBZffPG0adMmPXv2zO23356k5hQlW2yxRQqFQo3HK6+8kiSZOnVqDjjggFRVVaV9+/bp27dvJk2a1EhnBgAAAADQNOngXkgOPfTQzJ49O/fff3/atGmTZ555Jm3btq112xtuuCGzZ88uee1//vOfdO7cOUkyePDgtGrVKuPGjUuHDh3yxz/+MVtttVWef/75LLHEEt/J+QAAAAAAXyhUFBq7BOog4F5IXnvtteyyyy5Zc801kyQrrbRSndvOH1Kfc845ue+++/LPf/4zrVq1yj/+8Y888sgjeffdd1NZWZkk+d3vfpebbrop1113XQ488MBv90QAAAAAAMqEgHshOeKII3LwwQfnrrvuSr9+/bLLLrtkrbXWqvc148aNy/HHH59bb701q6yySpJk0qRJmT59epZccsmSbWfOnJmXXnqp1v3MmjUrs2bNKhlr8dmcVDZv9g3OCAAAAACgaTMH90JywAEH5OWXX85ee+2Vp59+Ouutt17OP//8Ord/5plnMnTo0PzmN79J//79q8enT5+erl275sknnyx5PPfcc/n5z39e675GjRqVDh06lDxGXV97GA4AAAAA8H1RKBaLxcYu4vto+PDhue222/LUU09lwoQJ2XLLLfPhhx+mY8eOmTJlSnr37p0+ffrksssuK3nd3XffnW222SYvvvhiunXr1qBj1drBfccQHdzfhbZtGruCRUOPDRq7gkVCYak1GruERUOltRS+EwX/DgQAgIbr1dgFNHmf9lutsUv4zrW859nGLqFBTFGykBx11FHZZpttssoqq+TDDz/M+PHj06NHj1q33WWXXdK6deuMHDkyb7/9dvV4VVVV+vXrl4022ig77rhjfvvb32aVVVbJm2++mdtuuy077bRT1ltvvRr7q6ysrJ6ve56icBsAAAAA+J4TcC8kc+bMyaGHHpr//e9/ad++fQYOHJhzzjmn1m3vv//+JMkKK6xQMj558uR069Ytt99+e0444YTsu+++ee+999KlS5dsvvnm6dy587d+HgAAAAAA5cIUJd9TxRu3bewSFg2mKPlumKLkO2GKku+IKUq+G6YoAQCABWCKkq9iipKmyyKTAAAAAACUJVOUAAAAAADUo1BRaOwSqIMObgAAAAAAypKAGwAAAACAsiTgBgAAAACgLAm4AQAAAAAoSxaZBAAAAACojzUmmywd3AAAAAAAlCUBNwAAAAAAZUnADQAAAABAWRJwAwAAAABQliwyCQAAAABQj4I24SbLrQEAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAehQqCo1dAnXQwQ0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCVzcAMAAAAA1KNgCu4mSwc3AAAAAABlScANAAAAAEBZEnADAAAAAFCWBNwAAAAAAJQli0wCAAAAANSjUGGVyaZKBzcAAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTAIAAAAA1EebcJPl1gAAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZUnADQAAAABAWbLIJAAAAABAfSoKjV0BddDBDQAAAABAWRJwAwAAAABQlgTcAAAAAACUJQE3AAAAAABlySKTAAAAAAD10SbcZLk1AAAAAACUJR3c31fdV27sChYN77zV2BUsEgqtOzV2CYuG2R81dgWLCL9b/k589nFjV7BoaNetsSsAAAAWcf4vGwAAAACAsqSDGwAAAACgPhWFxq6AOujgBgAAAACgLAm4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyZJFJAAAAAID6aBNustwaAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsCbgBAAAAAChLFpkEAAAAAKhPRaGxK6AOOrgBAAAAAChLAm4AAAAAAL6xP/zhD+nWrVtatmyZ3r1755FHHql3+6lTp+bQQw9N165dU1lZmVVWWSW33377Ah3TFCUAAAAAAHwj11xzTY455phcfPHF6d27d84999wMGDAgzz33XDp16lRj+9mzZ2frrbdOp06dct1112WZZZbJq6++mo4dOy7QcQXcAAAAAAB8I2effXZ+8pOfZN99902SXHzxxbntttty2WWX5fjjj6+x/WWXXZYPPvggDz74YJo3b54k6dat2wIf1xQlAAAAAAD1qSgseo8FMHv27Dz++OPp16/f/7tkFRXp169fHnrooVpfc8stt2SjjTbKoYcems6dO2eNNdbI6aefnjlz5izQsXVwAwAAAABQYtasWZk1a1bJWGVlZSorK2tsO2XKlMyZMyedO3cuGe/cuXOeffbZWvf/8ssv57777suPf/zj3H777XnxxRdzyCGH5LPPPsuIESMaXKcObgAAAAAASowaNSodOnQoeYwaNWqh7X/u3Lnp1KlTLrnkkvTq1Su77bZbTjjhhFx88cULtB8d3AAAAAAAlBg+fHiOOeaYkrHaureTZKmllkqzZs3yzjvvlIy/88476dKlS62v6dq1a5o3b55mzZpVj/Xo0SNvv/12Zs+enRYtWjSoTh3cAAAAAACUqKysTPv27UsedQXcLVq0SK9evXLvvfdWj82dOzf33ntvNtpoo1pfs8kmm+TFF1/M3Llzq8eef/75dO3atcHhdiLgBgAAAACoX8Ui+FhAxxxzTC699NJcfvnl+e9//5uDDz44n3zySfbdd98kyd57753hw4dXb3/wwQfngw8+yJFHHpnnn38+t912W04//fQceuihC3RcU5QAAAAAAPCN7Lbbbnnvvfdy0kkn5e23387aa6+dO+64o3rhyddeey0VFf8vOV9uueVy55135uijj85aa62VZZZZJkceeWSOO+64BTpuoVgsFhfqmdAkFJ86srFLWDS881ZjV7BIKPTarrFLWDQs1rKxK1g0tOjY2BUsGj77uLErWDS069bYFQAAsFD0auwCmrw5e/+wsUv4zjW7YlJjl9AgpigBAAAAAKAsCbgBAAAAAChL5uAGAAAAAKhPRaGxK6AOOrgBAAAAAChLAm4AAAAAAMqSgBsAAAAAgLJkDm4AAAAAgPpoE26y3BoAAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCwJuAEAAAAAKEsWmQQAAAAAqE9FobEroA46uAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsiTgBgAAAACgLFlkEgAAAACgPtaYbLJ0cAMAAAAAUJYE3AAAAAAAlCUBNwAAAAAAZUnADQAAAABAWbLI5PfArFmzMmvWrJKxFrM/T2ULtxcAAAAAvrEKq0w2VTq4vwdGjRqVDh06lDxG/emxxi4LAAAAAOBbJeD+Hhg+fHimTZtW8hi+/3qNXRYAAAAAwLfKHBbfA5WVlamsrCwZK5qeBAAAAAD4ntPBXQbGjBmTQsE8PwAAAAAA89PmWwYmT56cPn36NHYZAAAAALBosshkkyXgLgPjxo3LBRdc0NhlAAAAAAA0KQLuMvDII480dgkAAAAAAE2OObgBAAAAAChLOrgBAAAAAOqjTbjJcmsAAAAAAChLAm4AAAAAAMqSgBsAAAAAgLIk4AYAAAAAoCxZZBIAAAAAoD4VhcaugDro4AYAAAAAoCwJuAEAAAAAKEsCbgAAAAAAypKAGwAAAACAsmSRSQAAAACAehS0CTdZbg0AAAAAAGVJwA0AAAAAQFkScAMAAAAAUJYE3AAAAAAAlCWLTAIAAAAA1Kei0NgVUAcd3AAAAAAAlCUBNwAAAAAAZUnADQAAAABAWRJwAwAAAABQliwyCQAAAABQH23CTZZbAwAAAABAWRJwAwAAAABQlgTcAAAAAACUJXNwAwAAAADUp6LQ2BVQBx3cAAAAAACUJQE3AAAAAABlScANAAAAAEBZEnADAAAAAFCWLDIJAAAAAFAfi0w2WTq4AQAAAAAoSwJuAAAAAADKkoAbAAAAAICyJOAGAAAAAKAsWWQSAAAAAKA+2oSbLLcGAAAAAICyJOAGAAAAAKAsCbgBAAAAAChLAm4AAAAAAMqSRSYBAAAAAOpTUWjsCqiDgPv7ajG39jvRY4PGrgAWns8/bewKFg1zpzR2BYuGz2c2dgWLhhYfNnYFi4bKxRu7AgAAaLJMUQIAAAAAQFla4IB75syZmTFjRvXPr776as4999zcddddC7UwAAAAAACozwIH3DvssEOuuOKKJMnUqVPTu3fvnHXWWdlhhx1y0UUXLfQCAQAAAACgNgsccD/xxBPZbLPNkiTXXXddOnfunFdffTVXXHFFzjvvvIVeIAAAAABAo6pYBB9lYoFLnTFjRtq1a5ckueuuu7LzzjunoqIiG264YV599dWFXiAAAAAAANRmgQPulVdeOTfddFNef/313Hnnnenfv3+S5N1330379u0XeoEAAAAAAFCbBQ64TzrppBx77LHp1q1bevfunY022ijJF93c66yzzkIvEAAAAAAAarPYgr5g1113zaabbpq33norP/zhD6vHt9pqq+y0004LtTgAAAAAAKjLAgfcSdKlS5d06dKlZGyDDTZYKAUBAAAAADQpFYXGroA6LHDA/cknn+Q3v/lN7r333rz77ruZO3duyfMvv/zyQisOAAAAAADqssAB9wEHHJC///3v2WuvvdK1a9cUCn57AQAAAADAd2+BA+5x48bltttuyyabbPJt1AMAAAAAAA2ywAH34osvniWWWOLbqAUAAAAAoOmpaOwCqMsC35pTTz01J510UmbMmPFt1AMAAAAAAA2ywB3cZ511Vl566aV07tw53bp1S/PmzUuef+KJJxZacQAAAAAAUJcFDrh33HHHb6EMAAAAAABYMAsccI8YMeLbqAMAAAAAABbIAgfcSTJ16tRcd911eemll/Lzn/88SyyxRJ544ol07tw5yyyzzMKuEQAAAACg8VQUGrsC6rDAAfdTTz2Vfv36pUOHDnnllVfyk5/8JEsssURuuOGGvPbaa7niiiu+jToBAAAAAKBExYK+4Jhjjsk+++yTF154IS1btqwe/9GPfpT7779/oRYHAAAAAAB1WeCA+9FHH81BBx1UY3yZZZbJ22+/vVCKAgAAAACAr7LAAXdlZWU++uijGuPPP/98qqqqFkpRAAAAAADwVRY44N5+++1zyimn5LPPPkuSFAqFvPbaaznuuOOyyy67LPQCAQAAAAAaVcUi+CgTC1zqWWedlenTp6dTp06ZOXNm+vTpk5VXXjnt2rXLr3/962+jRgAAAAAAqGGxBX1Bhw4dcvfdd2fixImZNGlSpk+fnnXXXTf9+vVLsVj8NmoEAAAAAIAaFjjgPvPMM/Pzn/88m2yySTbZZJPq8Tlz5mTPPffMX/7yl4VaIAAAAAAA1GaBpyg588wz86c//alkbM6cORk6dGiefPLJhVUXAAAAAADUa4E7uG+77bb0798/HTp0yK677prPP/88Q4YMybPPPpvx48d/GzUCAAAAADSeikJjV0AdFjjgXn/99XP99ddnxx13TIsWLfKnP/0pL774YsaPH5/OnTt/GzUCAAAAAEANCzxFSZL07ds3V1xxRXbZZZdMnjw5f//734XbAAAAAAB8pxrUwb3zzjvXOl5VVZWOHTvmwAMPrB674YYbFk5lAAAAAABQjwYF3B06dKh1fMCAAQu1GAAAAAAAaKgGBdyjR4/+tusAAAAAAGiaLDLZZC3wIpPzvPfee3nuueeSJKuuumqqqqoWWlEAAAAAAPBVFniRyU8++ST77bdfunbtms03///Yu+8wKav7b8DfWWDpTQXsYiHYUCM2xK6IUYnYsEWKxk4U0QSwgbGAHRVbbKix94IdxRIVRcVgRCUERKNgBZSywO55//Blfm6QlRXY2Ye97+uaS/aZZ2Y+88w6O/OZM+fsFDvttFOsvvrqcfTRR8fs2bOXR0YAAAAAAFhEpQvuvn37xksvvRSPP/54TJ8+PaZPnx6PPvpovPTSS3Haaactj4wAAAAAALCISk9R8uCDD8YDDzwQu+yyS37b3nvvHfXr149u3brFddddtyzzAQAAAAAUVqWHCVNVKv3QzJ49O1q1arXI9pYtW5qiBAAAAACAKlPpgrtDhw4xcODAmDt3bn7bnDlz4txzz40OHTos03AAAAAAALA4SzxFSa1ateKLL76IoUOHxl577RVrrrlmbL755hER8d5770W9evXimWeeWW5BVyStW7eOPn36RJ8+fQodBQAAAAAgs5Z4BHdKKSIi2rVrFxMmTIjBgwfHFltsEVtssUUMGTIkJkyYEJtssslyC5pFw4cPj2bNmhU6BgAAAADACqnSi0xGRDRo0CCOOeaYZZ0FAAAAAKD6KcoVOgGLUamC+6abbopGjRpVuM/JJ5+8VIFWFKNGjYpevXpFREQu9+P/AAMHDoxBgwZFxI+LdR511FFx//33R/PmzeOss86KY489Nn/5Tz/9NE477bR49tlno6ioKHbccce48soro3Xr1lV9VwAAAAAAqqVcWjj3yC8oKiqKNddcM2rVqrX4K8vl4j//+c8yC5dl8+bNi+uuuy7OOeec+OijjyIiolGjRtGoUaNo3bp1fP/993HeeefFnnvuGQ888ECceeaZ8cEHH0Tbtm1j/vz5sfnmm0eHDh2iT58+Ubt27Tj//PPj7bffjn/+859RXFz8i7efPjhted9FIiKarFboBDVCrkHLQkeAZafoV315ispaMKfQCWqGxusUOkHNULd5oRMAACu89oUOUO2VXbZjoSNUuaLTXil0hCVSqXfZY8aMiZYtFU1Lori4OJo2bRq5XC5WXXXVRc7fe++948QTT4yIiH79+sUVV1wRL774YrRt2zbuvffeKCsri5tuuik/+vvWW2+NZs2axahRo2LPPfes0vsCAAAAAFAdLXHBvbBoZdnYbLPN8v9eWIJ/+eWXERHx3nvvxb///e9o3LhxucvMnTs3Jk6cuMh1lZSURElJSbltxfMWRN1iowQBAAAAgBXXEjegSziTCUuoTp065X7O5XJRVlYWERE//PBDtG/fPu68885FLteiRYtFtg0ePDjOPffcctvOOXG7GHTS9sswMQAAAADUUEWFDsDiLHHBPXDgwF9cYJLyiouLo7S0tNKX23LLLePee++Nli1bRpMmTX5x/wEDBkTfvn3L3/Z/zq707QIAAAAAZMkSf/YwcODAaNCgwfLMssJp3bp1/PDDDzFy5Mj4+uuvY/bs2Ut0uSOOOCJWWWWV2G+//eKVV16JSZMmxahRo+Lkk0+Ozz77bJH969atG02aNCl3Mj0JAAAAALCiM7h+Odp+++3j+OOPj0MOOSRatGgRF1988RJdrkGDBvHyyy/H2muvHQcccEBstNFGcfTRR8fcuXOXaEQ3AAAAAEBNkEsm114hpQ9OK3SEmqHJaoVOUCPkGrQsdARYdop8w6ZKLJhT6AQ1Q+N1Cp2gZqjbvNAJAIAVXvtCB6j2yq7YsdARqlzRqa8UOsIS8S4bAAAAAKAiuVyhE7AYlZ6iZODAgfHJJ58sjywAAAAAALDEKl1wP/roo7H++uvH7rvvHnfddVeUlJQsj1wAAAAAAFChShfcY8eOjbfeeis22WSTOOWUU2LVVVeNE044Id56663lkQ8AAAAAAH5WpQvuiIjf/va3cdVVV8Xnn38eN998c3z22WfRsWPH2GyzzeLKK6+MGTNmLOucAAAAAABQzq8quBdKKcX8+fNj3rx5kVKK5s2bx7Bhw2KttdaKe++9d1llBAAAAAAonFwNPGXEryq433777ejdu3esttpqceqpp8Zvf/vbGD9+fLz00ksxYcKEuOCCC+Lkk09e1lkBAAAAACCv0gV3u3btYrvttotJkybFzTffHJ9++mkMGTIkNthgg/w+hx12WHz11VfLNCgAAAAAAPxU7cpeoFu3bnHUUUfFGmussdh9VllllSgrK1uqYAAAAAAAUJFKjeCeP39+DB8+PGbOnLm88gAAAAAAVC+5XM07ZUSlCu46derE3Llzl1cWAAAAAABYYpWeg/ukk06Kiy66KBYsWLA88gAAAAAAwBKp9Bzcb731VowcOTKeffbZaNeuXTRs2LDc+Q899NAyCwcAAAAAAItT6YK7WbNmceCBBy6PLAAAAAAAsMQqXXDfeuutyyMHAAAAAED1lJ01F2ucSs/BHRGxYMGCeP755+OGG26I77//PiIiPv/88/jhhx+WaTgAAAAAAFicSo/g/uSTT2KvvfaKKVOmRElJSXTq1CkaN24cF110UZSUlMT111+/PHICAAAAAEA5lR7Bfcopp8RWW20V3333XdSvXz+/ff/994+RI0cu03AAAAAAALA4lR7B/corr8Rrr70WxcXF5ba3bt06/vvf/y6zYAAAAAAAUJFKF9xlZWVRWlq6yPbPPvssGjduvExCAQAAAABUGzmrTFZXlZ6iZM8994yhQ4fmf87lcvHDDz/EwIEDY++9916W2QAAAAAAYLEqPYL7sssui86dO8fGG28cc+fOjcMPPzwmTJgQq6yyStx9993LIyMAAAAAACyi0gX3mmuuGe+9917cc8898c9//jN++OGHOProo+OII44ot+gkAAAAAAAsT5UuuCMiateuHX/4wx+WdRYAAAAAAFhilS64b7/99grP7969+68OAwAAAABQ7VR6JUOqSqUL7lNOOaXcz/Pnz4/Zs2dHcXFxNGjQQMENAAAAAECVqPRnD99991250w8//BAfffRR7LDDDhaZBAAAAACgyiyTwfVt2rSJIUOGLDK6GwAAAAAAlpdlNntM7dq14/PPP19WVwcAAAAAABWq9Bzcjz32WLmfU0rxxRdfxLBhw6Jjx47LLBgAAAAAQLWQyxU6AYtR6YK7a9eu5X7O5XLRokWL2G233eKyyy5bVrkAAAAAAKBClS64y8rKlkcOAAAAAAColF89B/fXX38dM2fOXJZZAAAAAABgiVWq4J4+fXqcdNJJscoqq0SrVq2iefPmseqqq8aAAQNi9uzZyysjAAAAAEDh5GrgKSOWeIqSb7/9Njp06BD//e9/44gjjoiNNtooIiI++OCDuPrqq+O5556LV199Nf75z3/GG2+8ESeffPJyCw0AAAAAAEtccP/1r3+N4uLimDhxYrRq1WqR8/bcc8848sgj49lnn42rrrpqmQcFAAAAAICfWuKC+5FHHokbbrhhkXI7ImLVVVeNiy++OPbee+8YOHBg9OjRY5mGBAAAAACA/7XEc3B/8cUXsckmmyz2/E033TSKiopi4MCByyQYAAAAAABUZIlHcK+yyioxefLkWHPNNX/2/EmTJkXLli2XWTAAAAAAgGohl6FVF2uYJR7B3blz5zjzzDNj3rx5i5xXUlISZ599duy1117LNBwAAAAAACxOpRaZ3GqrraJNmzZx0kknxYYbbhgppRg/fnxce+21UVJSErfffvvyzAoAAAAAAHlLXHCvueaa8frrr8eJJ54YAwYMiJRSRETkcrno1KlTDBs2LNZee+3lFhQAAAAAAH5qiQvuiIh11103nnrqqfjuu+9iwoQJERGxwQYbxEorrbRcwrEUGnpMqkKubpNCR6gZGq5W6AQ1Q8l3hU5QM8z9ttAJaobSRadUYzmYM63QCWqG3BLPKsjSKG5a6AQAAPwKlSq4F2revHlss802yzoLAAAAAED1Y43JastwEAAAAAAAMknBDQAAAABAJim4AQAAAADIJAU3AAAAAACZ9KsWmQQAAAAAqDFyVpmsrozgBgAAAAAgkxTcAAAAAABkkoIbAAAAAIBMUnADAAAAAJBJFpkEAAAAAKiIYcLVlocGAAAAAIBMUnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCZZZBIAAAAAoCK5XKETsBhGcAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJpmDGwAAAACgIqbgrraM4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSRaZBAAAAACoSM4qk9WVEdwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkKbgAAAAAAMskikwAAAAAAFbDGZPVlBDcAAAAAAJmk4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATLLIJAAAAABARawyWW0ZwQ0AAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQEWtMVluZGsE9fPjwaNasWaFjVErr1q1j6NChhY4BAAAAALDCMYJ7OXvrrbeiYcOGhY4BAAAAALDCUXAvZy1atCh0BAAAAACAFVKmpihZ6JlnnomNNtooGjVqFHvttVd88cUX+fN22WWX6NOnT7n9u3btGj179sz/3Lp16zj//POje/fu0ahRo1hnnXXisccei6+++ir222+/aNSoUWy22WYxZsyY/GUWTo/yxBNPRNu2baNBgwZx0EEHxezZs+O2226L1q1bR/PmzePkk0+O0tLScrf10ylKcrlc3HTTTbH//vtHgwYNok2bNvHYY4+Vy/vYY49FmzZtol69erHrrrvGbbfdFrlcLqZPn75Mjh8AAAAAUAlFuZp3yojMFdyzZ8+OSy+9NO644454+eWXY8qUKXH66adX+nquuOKK6NixY7z77ruxzz77xJFHHhndu3ePP/zhD/HOO+/E+uuvH927d4+UUrnbvuqqq+Kee+6Jp59+OkaNGhX7779/PPnkk/Hkk0/GHXfcETfccEM88MADFd72ueeeG926dYt//vOfsffee8cRRxwR3377bURETJo0KQ466KDo2rVrvPfee3HcccfFmWeeWen7BwAAAACwostcwT1//vy4/vrrY6uttoott9wyevfuHSNHjqz09ey9995x3HHHRZs2beKcc86JmTNnxtZbbx0HH3xw/OY3v4l+/frF+PHjY9q0aeVu+7rrrovf/va3sdNOO8VBBx0Ur776atx8882x8cYbx7777hu77rprvPjiixXeds+ePeOwww6LDTbYIC688ML44Ycf4s0334yIiBtuuCHatm0bl1xySbRt2zYOPfTQcqPPAQAAAAD4Uebm4G7QoEGsv/76+Z9XW221+PLLLyt9PZtttln+361atYqIiHbt2i2y7csvv4xVV131Z2+7VatW0bp162jUqFG5bb+U56e33bBhw2jSpEn+Mh999FFsvfXW5fbfZpttKry+kpKSKCkpKbetuGRB1K2buYcXAAAAAGCJZW4Ed506dcr9nMvlyk0jUlRUVO7niB9HXld0PblcbrHbysrKKrztn9v208ss6X34pctUZPDgwdG0adNyp8HXvvSrrw8AAAAAIAsyV3D/khYtWpRbdLK0tDTef//9AiaqnLZt25Zb3DIi4q233qrwMgMGDIgZM2aUOw04ceflGRMAAAAAao5cDTxlxApXcO+2224xYsSIGDFiRHz44YdxwgknxPTp0wsda4kdd9xx8eGHH0a/fv3i448/jvvuuy+GDx8eEf83qvx/1a1bN5o0aVLuZHoSAAAAAGBFt8IV3EcddVT06NEjunfvHjvvvHOst956seuuuxY61hJbd91144EHHoiHHnooNttss7juuuvizDPPjIgfi2wAAAAAAH6US/87YTXVzgUXXBDXX399fPrpp0t8mfTJBcsxEQvlGrQodISaocm6hU5QM5R8V+gENcPcbwudoGYonVfoBDVD/ZULnaBmaLBaoRPUDMVNC50AAAqofaEDVHvprs6FjlDlcoc/U+gIS8Q8FtXQtddeG1tvvXWsvPLK8Y9//CMuueSS6N27d6FjAQAAAABUKwruamjChAlx/vnnx7fffhtrr712nHbaaTFgwIBCxwIAAACAmmkxa+NReAruauiKK66IK664otAxAAAAAACqtRVukUkAAAAAAGoGBTcAAAAAAJmk4AYAAAAAIJPMwQ0AAAAAUBFrTFZbRnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCYpuAEAAAAAyCSLTAIAAAAAVCRnlcnqyghuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgkc3ADAAAAAFTEFNzVlhHcAAAAAABkkoIbAAAAAIBMUnADAAAAAJBJCm4AAAAAAJbaNddcE61bt4569erFtttuG2+++eYSXe6ee+6JXC4XXbt2rfRtKrgBAAAAACpSlKt5p0q69957o2/fvjFw4MB45513YvPNN4/OnTvHl19+WeHlJk+eHKeffnrsuOOOv+6h+VWXAgAAAACA/+/yyy+PY445Jnr16hUbb7xxXH/99dGgQYO45ZZbFnuZ0tLSOOKII+Lcc8+N9dZb71fdroIbAAAAAIBySkpKYubMmeVOJSUlP7vvvHnz4u2334499tgjv62oqCj22GOPeP311xd7G3/961+jZcuWcfTRR//qnApuAAAAAADKGTx4cDRt2rTcafDgwT+779dffx2lpaXRqlWrcttbtWoVU6dO/dnLvPrqq3HzzTfHjTfeuFQ5ay/VpQEAAAAAWOEMGDAg+vbtW25b3bp1l8l1f//993HkkUfGjTfeGKussspSXZeCGwAAAACgIpVfczHz6tatu8SF9iqrrBK1atWKadOmlds+bdq0WHXVVRfZf+LEiTF58uTo0qVLfltZWVlERNSuXTs++uijWH/99Zfotk1RAgAAAADAr1ZcXBzt27ePkSNH5reVlZXFyJEjo0OHDovsv+GGG8a4ceNi7Nix+dPvf//72HXXXWPs2LGx1lprLfFtG8ENAAAAAMBS6du3b/To0SO22mqr2GabbWLo0KExa9as6NWrV0REdO/ePdZYY40YPHhw1KtXLzbddNNyl2/WrFlExCLbf4mCGwAAAACApXLIIYfEV199Feecc05MnTo1tthii3j66afzC09OmTIlioqW/YQiuZRSWubXSsGlTy4odIQaIdegRaEj1AxN1i10gpqh5LtCJ6gZ5n5b6AQ1Q+m8QieoGeqvXOgENUOD1QqdoGYoblroBABQQO0LHaDaSw/uXegIVS534JOFjrBEjOAGAAAAAKhIrgauMpkRFpkEAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqIg1JqstI7gBAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMU3AAAAAAAZJJFJgEAAAAAKpKzymR1ZQQ3AAAAAACZZAT3Ciq9OKrQEWqE3BFnFzpCzVCrbqET1Ay16hU6Qc1Qd6VCJ6gZZv6n0AlqhiLPz1VizleFTlAzlHxX6AQ1Q+PWhU4AAKxgjOAGAAAAACCTjOAGAAAAAKiIKbirLSO4AQAAAADIJAU3AAAAAACZpOAGAAAAACCTFNwAAAAAAGSSRSYBAAAAACpSZJXJ6soIbgAAAAAAMknBDQAAAABAJim4AQAAAADIJAU3AAAAAACZZJFJAAAAAICK5CwyWV0ZwQ0AAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQEYtMVltGcAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJim4AQAAAADIJItMAgAAAABUxCKT1ZYR3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSebgBgAAAACoSM444erKIwMAAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQkaJcoROwGEZwAwAAAACQSQpuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgki0wCAAAAAFQkZ5HJ6soIbgAAAAAAMknBDQAAAABAJim4AQAAAADIJAU3AAAAAACZpOBehnbZZZfo06fPYs9v3bp1DB06tNLXO2jQoNhiiy1+dS4AAAAAYCnkimreKSNqFzpATfLWW29Fw4YNCx0DAAAAAGCFoOCuQi1atKjw/Pnz50edOnWqKA0AAAAAQLZlZ6x5RixYsCB69+4dTZs2jVVWWSXOPvvsSClFxKJTlORyubjuuuvi97//fTRs2DAuuOCCiIgYMmRItGrVKho3bhxHH310zJ07txB3BQAAAACgWlNwL2O33XZb1K5dO95888248sor4/LLL4+bbrppsfsPGjQo9t9//xg3blwcddRRcd9998WgQYPiwgsvjDFjxsRqq60W1157bRXeAwAAAACAbDBFyTK21lprxRVXXBG5XC7atm0b48aNiyuuuCKOOeaYn93/8MMPj169euV/PvTQQ+Poo4+Oo48+OiIizj///Hj++eeN4gYAAACAQsnlCp2AxTCCexnbbrvtIveTX/gOHTrEhAkTorS09Gf332qrrcr9PH78+Nh2223LbevQoUOFt1lSUhIzZ84sdyqZX/Yr7wEAAAAAQDYouAusYcOGS30dgwcPjqZNm5Y7DRkxaRmkAwAAAACovhTcy9jo0aPL/fzGG29EmzZtolatWkt0+Y022uhnr6MiAwYMiBkzZpQ79d9n3coFBwAAAADIGHNwL2NTpkyJvn37xnHHHRfvvPNOXH311XHZZZct8eVPOeWU6NmzZ2y11VbRsWPHuPPOO+Nf//pXrLfeeou9TN26daNu3brltpXV8dkFAAAAACwTRebgrq4U3MtY9+7dY86cObHNNttErVq14pRTToljjz12iS9/yCGHxMSJE+Mvf/lLzJ07Nw488MA44YQT4plnnlmOqQEAAAAAsieXUkqFDsGyVza8U6Ej1AhFR5xd6Ag1Q626v7wPS69sXqET1AylJYVOUDPM/E+hE9QMdVcqdIKaIf38YuUsY0XG/lSJxq0LnQCAn9W+0AGqvfTCoYWOUOVyu91T6AhLxDwWAAAAAABkkoIbAAAAAIBM8j08AAAAAICK5IwTrq48MgAAAAAAZJKCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyySKTAAAAAAAVyeUKnYDFMIIbAAAAAIBMUnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCZZZBIAAAAAoCIWmay2jOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqEjOOOHqyiMDAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMsMgkAAAAAUJGiXKETsBhGcAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJpmDGwAAAACgIjlzcFdXRnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCYpuAEAAAAAyCSLTAIAAAAAVCRnnHB15ZEBAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqEguV+gELIYR3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyySKTAAAAAAAVKbLIZHWl4F5BFe21X6Ej1Ayzvyh0gpqhwWqFTlAzlM0rdIKaIZUVOkGNkL7/vNARaoa50wudoEbINVy10BFqBq83qkZpSaET1Ay16hY6AQBUGVOUAAAAAACQSQpuAAAAAAAyScENAAAAAEAmmYMbAAAAAKAiOeOEqyuPDAAAAAAAmaTgBgAAAAAgkxTcAAAAAABkkjm4AQAAAAAqkssVOgGLYQQ3AAAAAACZpOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgEyyyCQAAAAAQEUsMlltGcENAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMsMgkAAAAAUBGLTFZbRnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCYpuAEAAAAAyCSLTAIAAAAAVKTIOOHqyiMDAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMsMgkAAAAAUJFcrtAJWAwjuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgk8zBDQAAAABQEXNwV1tGcAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJim4AQAAAADIJItMAgAAAABUJGeccHXlkQEAAAAAIJMU3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSRaZBAAAAACoSFGu0AlYDCO4l4N//OMfscsuu0SDBg2iefPm0blz5/juu+8iIuLpp5+OHXbYIZo1axYrr7xy7LvvvjFx4sT8ZSdPnhy5XC4eeuih2HXXXaNBgwax+eabx+uvv16ouwMAAAAAUC0puJexsWPHxu677x4bb7xxvP766/Hqq69Gly5dorS0NCIiZs2aFX379o0xY8bEyJEjo6ioKPbff/8oKysrdz1nnnlmnH766TF27Nj4zW9+E4cddlgsWLCgEHcJAAAAAKBayqWUUqFDrEgOP/zwmDJlSrz66qtLtP/XX38dLVq0iHHjxsWmm24akydPjnXXXTduuummOProoyMi4oMPPohNNtkkxo8fHxtuuOGSBZk67NfeBSqjfotCJ6gZGqxW6AQ1Q9m8QieoGVLZL+/DUkufv1boCDVDnQaFTlAj5BquWugINUPj1oVOUDMUmSWzStSqW+gEQOa0L3SAai+936fQEapcbtOhhY6wRIzgXsYWjuBenAkTJsRhhx0W6623XjRp0iRat24dERFTpkwpt99mm22W//dqq/1Y7n355Zc/e50lJSUxc+bMcqeSkvlLeU8AAAAAAKo3BfcyVr9+/QrP79KlS3z77bdx4403xujRo2P06NERETFvXvmRk3Xq1Mn/O5f7cRL7/53GZKHBgwdH06ZNy50GX/3c0twNAAAAAGChXK7mnTJCwb2MbbbZZjFy5MifPe+bb76Jjz76KM4666zYfffdY6ONNsovPrk0BgwYEDNmzCh3GvCnTkt9vQAAAAAA1ZkJ0JaxAQMGRLt27eLEE0+M448/PoqLi+PFF1+Mgw8+OFZaaaVYeeWV429/+1usttpqMWXKlOjfv/9S32bdunWjbt3/mWNtdp2f3xkAAAAAYAVhBPcy9pvf/CaeffbZeO+992KbbbaJDh06xKOPPhq1a9eOoqKiuOeee+Ltt9+OTTfdNE499dS45JJLCh0ZAAAAACCTcimlVOgQLAdThxU6Qc1Qv0WhE9QMDVYrdIKaoWzeL+/D0ks/v54Cy1b6/LVCR6gZ6jQodIIaIddw1UJHqBkaty50gpqhyJeIq0Stur+8D0A57QsdoNpL/zq10BGqXG6TKwodYYl4dQEAAAAAUJGciTCqK48MAAAAAACZpOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgEyyyCQAAAAAQEVyuUInYDGM4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATDIHNwAAAABARczBXW0ZwQ0AAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQkSLjhKsrjwwAAAAAAJmk4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATLLIJAAAAABAhXKFDsBiGMENAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMsMgkAAAAAUJGcRSarKyO4AQAAAADIJAU3AAAAAACZpOAGAAAAACCTFNwAAAAAAGSSRSYBAAAAACqSM064uvLIAAAAAACQSQpuAAAAAAAyScENAAAAAEAmmYMbAAAAAKBCuUIHYDGM4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSRaZBAAAAACoSM4ik9WVEdwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkKbgAAAAAAMskikyuotGBuoSPUCLnaDQodoWYo8lRVJcpKCp2gZih1nKtCruUWhY5QM/h9rho//LfQCWqGolqFTlAz1GlS6AQ1Q72VCp2gZqhVr9AJgKqUM064uvLIAAAAAACQSQpuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgkK7cBAAAAAFQoV+gALIYR3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyySKTAAAAAAAVyVlksroyghsAAAAAgExScAMAAAAAkEkKbgAAAAAAMskc3AAAAAAAFTJOuLryyAAAAAAAkEkKbgAAAAAAMknBDQAAAABAJim4AQAAAADIJItMAgAAAABUJJcrdAIWwwhuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJlkkUkAAAAAgIpYZLLaMoIbAAAAAIBMUnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCZZZBIAAAAAoEIWmayujOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAsNSuueaaaN26ddSrVy+23XbbePPNNxe774033hg77rhjNG/ePJo3bx577LFHhfsvjoIbAAAAAKAiuaKad6qke++9N/r27RsDBw6Md955JzbffPPo3LlzfPnllz+7/6hRo+Kwww6LF198MV5//fVYa621Ys8994z//ve/lXtoUkqp0mmp9tJnlxY6Qo2Qa9620BFqhnorFzpBzbBgVqET1AwL5hY6Qc2QSgudoGYoLSl0gprhh8q9wOdXatiq0AlqhjpNCp2gZqi3UqET1Ay16hU6ASxD7QsdoNpLU4YUOkKVy63dv1L7b7vttrH11lvHsGHDIiKirKws1lprrfjTn/4U/fv/8nWVlpZG8+bNY9iwYdG9e/clvl0juAEAAAAA+NXmzZsXb7/9duyxxx75bUVFRbHHHnvE66+/vkTXMXv27Jg/f36stFLlPqitXam9AQAAAABY4ZWUlERJSflvbdatWzfq1q27yL5ff/11lJaWRqtW5b8V16pVq/jwww+X6Pb69esXq6++ermSfEkYwQ0AAAAAUJFcrsadBg8eHE2bNi13Gjx48HI5vEOGDIl77rknHn744ahXr3JTQBnBDQAAAABAOQMGDIi+ffuW2/Zzo7cjIlZZZZWoVatWTJs2rdz2adOmxaqrrlrh7Vx66aUxZMiQeP7552OzzTardE4juAvgkUceiQ022CBq1aoVffr0ieHDh0ezZs0KHQsAAAAAICJ+LLObNGlS7rS4gru4uDjat28fI0eOzG8rKyuLkSNHRocOHRZ7GxdffHGcd9558fTTT8dWW231q3IquAvguOOOi4MOOig+/fTTOO+88wodBwAAAABgqfTt2zduvPHGuO2222L8+PFxwgknxKxZs6JXr14REdG9e/cYMGBAfv+LLroozj777LjllluidevWMXXq1Jg6dWr88MMPlbpdU5RUsR9++CG+/PLL6Ny5c6y++uqFjgMAAAAAsNQOOeSQ+Oqrr+Kcc86JqVOnxhZbbBFPP/10fuHJKVOmRFHR/423vu6662LevHlx0EEHlbuegQMHxqBBg5b4dhXcVWjUqFGx6667RkTEbrvtFhERL774Yrl9Jk+eHOutt168+eab5YblDx06NK644oqYNGlSuV8EAAAAAGB5yxU6QCb07t07evfu/bPnjRo1qtzPkydPXia3qSmtQttvv3189NFHERHx4IMPxhdffBHbb799uX1at24de+yxR9x6663ltt96663Rs2dP5TYAAAAAwP+nLa1CxcXF0bJly4iIWGmllWLVVVeN4uLiRfb74x//GHfffXeUlJRERMQ777wT48aNy89XAwAAAACAgrta6tq1a9SqVSsefvjhiIgYPnx47LrrrtG6deuf3b+kpCRmzpxZ7lRSsqAKEwMAAAAAVD0FdzVUXFwc3bt3j1tvvTXmzZsXd911Vxx11FGL3X/w4MHRtGnTcqfB17xQhYkBAAAAAKqeRSarqT/+8Y+x6aabxrXXXhsLFiyIAw44YLH7DhgwIPr27VtuW/FX1y7viAAAAABQM+SME66uFNzV1EYbbRTbbbdd9OvXL4466qioX7/+YvetW7du1K1bt9y2NNNDCwAAAACs2Hz0UI0dffTRMW/evAqnJwEAAAAAqKkU3FWsWbNmkVKKXXbZJb+tZ8+eMX369EX2/e9//xvt2rWLrbfeuuoCAgAAAABkhIK7Gvrhhx/i/fffj2HDhsWf/vSnQscBAAAAAKiWTNRcDfXu3Tvuvvvu6Nq1q+lJAAAAAKDAcrlcoSOwGAruamj48OExfPjwQscAAAAAAKjWTFECAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJlkDm4AAAAAgApZZLK6MoIbAAAAAIBMUnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCZZZBIAAAAAoCI544SrK48MAAAAAACZpOAGAAAAACCTFNwAAAAAAGSSObgBAAAAACqUK3QAFsMIbgAAAAAAMknBDQAAAABAJim4AQAAAADIJAU3AAAAAACZZJFJAAAAAICK5CwyWV0ZwQ0AAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQkZxxwtWVRwYAAAAAgExScAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJllkEgAAAACgQrlCB2AxjOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqEjOIpPVlRHcAAAAAABkkoIbAAAAAIBMUnADAAAAAJBJ5uAGAAAAAKhIzjjh6sojAwAAAABAJim4AQAAAADIJFOUrKBy9VcqdARYdnK1Cp2gZsj5k1AlatUtdAJYdmrXK3QCWHZyuUInqBHSt+MLHaFGyDVeo9ARaob6rQqdoGao27zQCYBqzghuAAAAAAAyyXA9AAAAAIAK+bZXdWUENwAAAAAAmaTgBgAAAAAgkxTcAAAAAABkkoIbAAAAAIBMssgkAAAAAEBFchaZrK6M4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSRaZBAAAAACokHHC1ZVHBgAAAACATFJwAwAAAACQSQpuAAAAAAAyScENAAAAAEAmWWQSAAAAAKAiuVyhE7AYRnADAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCaZgxsAAAAAoCLm4K62jOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqJBxwtWVRwYAAAAAgExScAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJllkEgAAAACgIrlcoROwGEZwAwAAAACQSQpuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgki0wCAAAAAFTIIpPVlRHcAAAAAABkkoIbAAAAAIBMUnADAAAAAJBJCm4AAAAAADKpWhXcw4cPj2bNmi3X2xg1alTkcrmYPn36cr2dX6Nnz57RtWvXQscAAAAAAH4qV1TzThmRnaQAAAAAAPATCu4CmD9/fqEjAAAAAABkXrUsuB955JFo06ZN1KtXLzp37hyffvpp/ryJEyfGfvvtF61atYpGjRrF1ltvHc8//3y5y5eUlES/fv1irbXWirp168YGG2wQN99888/e1uzZs+N3v/tddOzYMaZPnx4HHXRQ9O7dO39+nz59IpfLxYcffhgREfPmzYuGDRvmb/Ppp5+OHXbYIZo1axYrr7xy7LvvvjFx4sT85SdPnhy5XC7uvffe2HnnnaNevXpx5513RmlpafTt2zd/ub/85S+RUiqX7YEHHoh27dpF/fr1Y+WVV4499tgjZs2atXQHFwAAAABgBVHtCu7Zs2fHBRdcELfffnv84x//iOnTp8ehhx6aP/+HH36IvffeO0aOHBnvvvtu7LXXXtGlS5eYMmVKfp/u3bvH3XffHVdddVWMHz8+brjhhmjUqNEitzV9+vTo1KlTlJWVxXPPPRfNmjWLnXfeOUaNGpXf56WXXopVVlklv+2tt96K+fPnx/bbbx8REbNmzYq+ffvGmDFjYuTIkVFUVBT7779/lJWVlbut/v37xymnnBLjx4+Pzp07x2WXXRbDhw+PW265JV599dX49ttv4+GHH87v/8UXX8Rhhx0WRx11VIwfPz5GjRoVBxxwwCIlOAAAAABATZVL1agxHT58ePTq1SveeOON2HbbbSMi4sMPP4yNNtooRo8eHdtss83PXm7TTTeN448/Pnr37h0ff/xxtG3bNp577rnYY489Ftl31KhRseuuu8b48ePjkEMOiTZt2sRdd90VxcXFERExbty42HzzzWPatGlRu3btWHXVVePss8+O999/P+6555644IIL4sknn4x//OMfP5vl66+/jhYtWsS4ceNi0003jcmTJ8e6664bQ4cOjVNOOSW/3+qrrx6nnnpq/PnPf46IiAULFsS6664b7du3j0ceeSTeeeedaN++fUyePDnWWWedyh/Mb26p/GWovHotCp2gZqjfstAJaoYFswudoGYoM00VK5KyX96FpVcyo9AJaoZcrtAJaoQ0Y3KhI9QIucZrFDpCzVC/VaET1Ax1mxc6QQ3RvtABqr/pdxY6QdVrdkShEyyRajeCu3bt2rH11lvnf95www2jWbNmMX78+Ij4cQT36aefHhtttFE0a9YsGjVqFOPHj8+P4B47dmzUqlUrdt555wpvp1OnTrHBBhvEvffemy+3I34sy1daaaV46aWX4pVXXonf/va3se+++8ZLL70UET+O6N5ll13y+0+YMCEOO+ywWG+99aJJkybRunXriIhyI8ojIrbaaqv8v2fMmBFffPFFvsRfeL9/us/mm28eu+++e7Rr1y4OPvjguPHGG+O777772ftSUlISM2fOLHcqKVGgAAAAAAArtmpXcP+S008/PR5++OG48MIL45VXXomxY8dGu3btYt68eRERUb9+/SW6nn322Sdefvnl+OCDD8ptz+VysdNOO8WoUaPyZfZmm20WJSUl8f7778drr71Wrjzv0qVLfPvtt3HjjTfG6NGjY/To0RER+TwLNWzYsFL3s1atWvHcc8/FU089FRtvvHFcffXV0bZt25g0adIi+w4ePDiaNm1a7jR46JOVuj0AAAAAgKypdgX3ggULYsyYMfmfP/roo5g+fXpstNFGERHxj3/8I3r27Bn7779/tGvXLlZdddWYPHlyfv927dpFWVlZfsT14gwZMiR69OgRu++++yIl98J5uEeNGhW77LJLFBUVxU477RSXXHJJlJSURMeOHSMi4ptvvomPPvoozjrrrNh9991jo402Wuwo659q2rRprLbaavkyfOH9fvvtt8vtl8vlomPHjnHuuefGu+++G8XFxeXm6V5owIABMWPGjHKnAX32/sUcAAAAAABZVrvQAf5XnTp14k9/+lNcddVVUbt27ejdu3dst912+fm327RpEw899FB06dIlcrlcnH322eUWdGzdunX06NEjjjrqqLjqqqti8803j08++SS+/PLL6NatW7nbuvTSS6O0tDR22223GDVqVGy44YYREbHLLrvEqaeeGsXFxbHDDjvkt51++umx9dZb50djN2/ePFZeeeX429/+FquttlpMmTIl+vfvv0T385RTTokhQ4ZEmzZtYsMNN4zLL788pk+fnj9/9OjRMXLkyNhzzz2jZcuWMXr06Pjqq6/yRf9P1a1bN+rWrVt+4/w6S5QDAAAAAPgl1uuorqrdCO4GDRpEv3794vDDD4+OHTtGo0aN4t57782ff/nll0fz5s1j++23jy5dukTnzp1jyy23LHcd1113XRx00EFx4oknxoYbbhjHHHNMzJo162dv74orrohu3brFbrvtFh9//HFE/DgKvFmzZrHFFltEo0aNIuLHgru0tLTc/NtFRUVxzz33xNtvvx2bbrppnHrqqXHJJZcs0f087bTT4sgjj4wePXpEhw4donHjxrH//vvnz2/SpEm8/PLLsffee8dvfvObOOuss+Kyyy6L3/3ud0t0/QAAAAAAK7pcSikVOgTLwTe3FDpBzVCvRaET1Az1WxY6Qc2wYHahE9QMZRYBZkVS9su7sPRKZhQ6Qc2QMyqrKqQZkwsdoUbINV6j0BFqhvqtCp2gZqjbvNAJaoj2hQ5Q/U2/q9AJql6zwwudYIlUuxHcAAAAAACwJBTcAAAAAABkUrVbZBIAAAAAoFrJGSdcXXlkAAAAAADIJAU3AAAAAACZpOAGAAAAACCTFNwAAAAAAGSSRSYBAAAAACqUK3QAFsMIbgAAAAAAMknBDQAAAABAJim4AQAAAADIJAU3AAAAAACZZJFJAAAAAICK5CwyWV0ZwQ0AAAAAQCYpuAEAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkywyCQAAAABQIeOEqyuPDAAAAAAAmaTgBgAAAAAgkxTcAAAAAABkkjm4AQAAAAAqkssVOgGLYQQ3AAAAAACZpOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgEyyyCQAAAAAQEVyxglXVx4ZAAAAAAAyScENAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJlkkUkAAAAAgArlCh2AxTCCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyScENAAAAAEAmWWQSAAAAAKAiOYtMVldGcAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJim4AQAAAADIJItMAgAAAABUyDjh6krBvaJKqdAJaoYFswqdoGYom1foBDVDrlahE9QMRZ6fq0RpSaET1AxFXkpWieImhU5QM3j9XCVyjdcsdISawfNG1SidW+gENcNX7xY6Qc3Qon2hE8Cv5qMHAAAAAAAyScENAAAAAEAm+V4pAAAAAEBFcrlCJ2AxjOAGAAAAACCTFNwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkWmQQAAAAAqJBxwtWVRwYAAAAAgExScAMAAAAAkEkKbgAAAAAAMknBDQAAAABAJllkEgAAAACgIrlcoROwGEZwAwAAAACQSQpuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgki0wCAAAAAFTEIpPVlhHcAAAAAABkkoIbAAAAAIBMUnADAAAAAJBJCm4AAAAAADLJIpMAAAAAABUyTri68sgAAAAAAJBJCm4AAAAAADJJwQ0AAAAAQCYpuAEAAAAAyCSLTAIAAAAAVCSXK3QCFsMIbgAAAAAAMknBDQAAAABAJim4AQAAAADIJHNwAwAAAABUyBzc1ZUR3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyySKTAAAAAAAVyRknXF15ZAAAAAAAyCQFNwAAAAAAmaTgBgAAAAAgkxTcAAAAAABkkkUmAQAAAAAqlCt0ABbDCG4AAAAAADJJwQ0AAAAAQCYpuAEAAAAAyCRzcK8ASkpKoqSkpNy2uiXzo27dOgVKBAAAAACw/BnBvQIYPHhwNG3atNxp8JVPFjoWAAAAAKwYckU175QRuZRSKnQIls7PjuD+/i4juKtCnYaFTlAzNFyj0AlqBn8OqkZaUOgENUNpyS/vw9Ir8mXAKlHmeaNK+DtYNeZNL3SCmqG4SaETwLIza2qhE9QMLf5Y6ATV3/yXCp2g6tXZudAJlkh2qvgabtiwYbH77rv/7Hl169aNJk2alDsptwEAAACAFZ2COyO+/vrrmDhxYqFjAAAAAABUGwrujBg0aFBMnjy50DEAAAAAAKoNEycCAAAAAFQoV+gALIYR3AAAAAAAZJKCGwAAAACATFJwAwAAAACQSebgBgAAAACoSM4c3NWVEdwAAAAAAGSSghsAAAAAgExScAMAAAAAkEkKbgAAAAAAMskikwAAAAAAFckZJ1xdeWQAAAAAAMgkBTcAAAAAAJmk4AYAAAAAIJMU3AAAAAAAZJJFJgEAAAAAKpQrdAAWwwhuAAAAAAAyScENAAAAAEAmKbgBAAAAAMgkBTcAAAAAAJlkkUkAAAAAgIrkLDJZXRnBDQAAAABAJim4AQAAAADIJAU3AAAAAACZpOAGAAAAACCTLDIJAAAAAFAh44SrK48MAAAAAACZpOAGAAAAACCTFNwAAAAAAGSSObgBAAAAACqSyxU6AYthBDcAAAAAAJmk4AYAAAAAIJMU3AAAAAAAZJKCGwAAAACATLLIJAAAAABAhYwTrq48MgAAAAAAZJKCGwAAAACATFJwAwAAAACQSQpuAAAAAAAyScENAAAAAFCRXK7mnX6Fa665Jlq3bh316tWLbbfdNt58880K97///vtjww03jHr16kW7du3iySefrPRtKrgBAAAAAFgq9957b/Tt2zcGDhwY77zzTmy++ebRuXPn+PLLL392/9deey0OO+ywOProo+Pdd9+Nrl27RteuXeP999+v1O3mUkppWdwBqpmvby50gpqhTsNCJ6gZGq5R6AQ1gz8HVSMtKHSCmqG0pNAJaoai2oVOUDOUed6oEv4OVo150wudoGYoblLoBLDszJpa6AQ1Q4s/FjpBBrxd6AAF0L5Se2+77bax9dZbx7BhwyIioqysLNZaa63405/+FP37919k/0MOOSRmzZoVTzzxRH7bdtttF1tssUVcf/31S3y7RnADAAAAAPCrzZs3L95+++3YY4898tuKiopijz32iNdff/1nL/P666+X2z8ionPnzovdf3EMuwEAAAAAoJySkpIoKSn/7di6detG3bp1F9n366+/jtLS0mjVqlW57a1atYoPP/zwZ69/6tSpP7v/1KmV++aGgntFtcrRhU5QKSUlJTF48OAYMGDAz/5PwrLhOFcNx7lqOM5Vw3GuGo5z1XCcq4bjXDUye5wbFDpA5WT2OGeM41w1MnucPW9QbVRuuo4VweDBg+Lcc88tt23gwIExaNCgwgRaDHNwUy3MnDkzmjZtGjNmzIgmTcwXt7w4zlXDca4ajnPVcJyrhuNcNRznquE4Vw3HuWo4zlXDca4ajnPVcJxZkVRmBPe8efOiQYMG8cADD0TXrl3z23v06BHTp0+PRx99dJHLrL322tG3b9/o06dPftvAgQPjkUceiffee2+Jc5qDGwAAAACAcurWrRtNmjQpd1rcNxOKi4ujffv2MXLkyPy2srKyGDlyZHTo0OFnL9OhQ4dy+0dEPPfcc4vdf3FMUQIAAAAAwFLp27dv9OjRI7baaqvYZpttYujQoTFr1qzo1atXRER079491lhjjRg8eHBERJxyyimx8847x2WXXRb77LNP3HPPPTFmzJj429/+VqnbVXADAAAAALBUDjnkkPjqq6/inHPOialTp8YWW2wRTz/9dH4hySlTpkRR0f9NKLL99tvHXXfdFWeddVacccYZ0aZNm3jkkUdi0003rdTtKripFurWrRsDBw60AMNy5jhXDce5ajjOVcNxrhqOc9VwnKuG41w1HOeq4ThXDce5ajjOVcNxpqbr3bt39O7d+2fPGzVq1CLbDj744Dj44IOX6jYtMgkAAAAAQCZZZBIAAAAAgExScAMAAAAAkEkKbgAAAAAAMknBDUC1U1ZWVugIAAAAQAYouAF+BevzLh+XXHJJvPjii1FUVOQYA1QjPnisWv4GsqJY+Nzhd7rqONZATaTgBlgCC18o/ve//42IiFwuV8g4K6Q5c+bEa6+9Fp07d45//OMfkcvlvEBfThxXoLKKin582zBlypSI8DyyvIwbNy4ivM6oaj7AWX4WPnd88sknBU6y4lr4fPzhhx/GrFmzPH8ANZKCGzLup28wvdlcPlJKkcvl4vHHH48jjjgibrvttkJHWuGUlpZG/fr148Ybb4wjjjgi9txzz3j11VeV3MvBwt/nl156KYYMGRLdu3ePkSNHxmeffVboaPCreI5Yvn5a/L3wwgvRunXreP311z0/LwfPPPNM7L777nHLLbcUOkqNMGHChBg7dmx8+umn+RKW5WPEiBGx/fbb5z8gY9lZ+Lru0Ucfjc6dO8d1110X8+bNK3QsgCrnLzlkWFlZWblP6OfPn1/ANCuuXC4XDz/8cHTr1i3233//2Gabbcqd7w3+0rn00kujW7duMWfOnFhllVXi0ksvjQMPPDA6d+6s5F4OcrlcPPTQQ7HffvvFv/71r5g/f3706NEjBg4cGF999VWh40GlLHxj/8ILL0Tfvn1j//33j2uuucYHNstIWVlZvvi78cYbY/To0RER8fvf/97z83Kw+uqrx4EHHhiXXXZZ3HrrrYWOs0J76KGHYrvttosDDzwwNt5447j99tu9jl6OGjRoEM2aNct/E9KI+aVXWloaEZEfhHPYYYfFgAED4oADDoji4uICpwOoegpulqufe9PjBc2y8dM3nVdddVV07949dtppp7j66qtj8uTJhQ23gpkyZUqcc845cfnll8cpp5wSbdq0iZKSknjuuedixowZvga4lNq1axcjRoyI3r17x5w5c2LllVeOK664Qsm9nEyYMCH69esXl112Wdxxxx1xxx13xLRp02KNNdaIFi1aFDoeVMrCDyD333//mD59emy55ZZx2mmnRd++fZXcy8DC1xn9+/ePgQMHRosWLeLcc8+Ndu3aRefOnePll1/2/LwMtWvXLvr16xd77bVXXHrppXH33XcXOtIK6dNPP40zzjgjBg8eHHfffXecdtpp0atXr7jmmmuipKSk0PEyLaX0s+/1dt1111h//fXj9NNPj4gwYn4pjBw5MubMmRO1atWKsrKy+P7772PYsGExYMCAOP7442ONNdaIadOmxfXXXx9vvPFGfPPNN4WODFAl/GVhuVj4Rmdh8Tdz5sz8nIJe0CwbP33TOWTIkFhvvfWiR48eccopp8SQIUPiu+++K3DC7PnpC/IFCxbk/53L5eL777+P9u3bR1lZWVx66aWx2267xYEHHhi//e1vfd1yKXXu3DmefvrpuO++++KEE05Qci9nc+bMiWbNmsXRRx8dH330Uay77rrRs2fP+Otf/xoREf/617+8wSczPv300zj77LNj8ODBccstt8RZZ50V9erVi9atW8eaa65Z6HgrhE8//TQefvjhuPzyy+OPf/xjnH322TF8+PDo2rVr/O53v4vXXnstcrmcAQxLaeFozJkzZ0bDhg3j+++/j759+yq5l7GRI0fGE088EZ06dYpjjz02ttlmmxg0aFBcfPHF0bdv37juuuv8DVwKuVwu/x5l1qxZ5c4744wzoqSkJJ599tmI8A3IX+OVV16Jk046KQYMGBBz586NoqKimDt3bnzyySfRvHnzmDlzZpx99tlx8MEHR//+/ePggw+ORx99NCIcb2DFp2lkmfvptBllZWVx4403Rvfu3WPzzTePm2++ucDpVixvvPFGPPDAA/HQQw/FoEGDYtttt41cLhfbb799NG/evNDxMqeoqCg+/fTT+OSTT6J27drxyCOPxLXXXhtFRUWx6aabRo8ePWLttdeON954I/bZZ5+YMGFCpJTiuuuuK3T0zNtll13i0UcfjQcffHCxJbeFJysvpZQ/Xgs/tJk2bVrMnDkzJk+eHL/73e/id7/7Xdxwww0REfHaa6/F0KFD4/PPPy9Y5prC7/GyUVZWFg0aNIhjjjkmJk6cGGuuuWZ069YtLr744oiIePvttwucMPvmzp0bU6ZMiWbNmuW3rbXWWjFw4MBYbbXVokuXLjF69OgoKipSci+FWrVqxUMPPRQ77rhjlJSURNeuXaNVq1ZxzjnnxPDhwwsdb4Xx2GOPxUknnRSvvvpqzJgxI7/9tNNOi0suuST69+8fl19+uZK7knr27Bl//vOf8z9fe+210aZNmxg0aFB88MEHEfHjNxSKi4vjoYceiggLqf4a7du3j4MPPjhGjx4dAwYMiDlz5kSLFi1i//33j9NOOy1at24dEyZMiD/84Q/5bzU99dRTEeF4L2+TJk0qdAQgwXIwe/bsdO6556a99tortWrVKh155JFpnXXWSaNHjy50tExbsGBBuZ9feOGF1LFjx5RSSvfee29q1KhRuu6661JKKc2YMSP94x//qPKMWVRWVpZSSmn69Ompa9euaZtttknXXHNNyuVy6d57700ppTR69Oh05ZVXpiuuuCJNmzYtf5kuXbqkYcOGFSx71i08jguNHDkyNWzYMPXo0SPNnj07pZTS119/nXr16pVyuVx6/fXXCxEzk8rKyvLH99FHH02HHnpo/rwdd9wx5XK5dNRRR5W7TL9+/dKOO+6YvvzyyyrNWhNNnTq10BFWCOPHj09rrLFGevzxx9P666+fjjnmmPzfyvfeey/tu+++6d133y1syAwpLS392e2dOnVKhx9+ePruu+/K7du1a9fUtm3b1KhRozR+/PgqSrli+vbbb9N2222Xzj333Py2d999Nx177LFp/fXXT3fddVcB0604ysrK0qBBg1KtWrXSnXfeucj55513XlpppZXSN998U4B02fTUU0+l8847LzVv3jxdcMEFKaUf34f07ds3denSJTVo0CCdfvrpafTo0enVV19Nq666anrttdcKnDp75s+fn//3oEGD0m677ZZOP/30NHfu3JTSj4/Dgw8+mObNm5ff95hjjkl/+tOfyl2WZe+FF15IDRo0SI8++miho0CNpuBmmRozZky6+OKLU+vWrdP222+f/vrXv6a5c+emXr16pd12222RMosl98MPP+T//a9//SullNKoUaPSuuuum2644YbUtGnTdO211+b3eeqpp9K+++6b/vOf/1R51qx5+eWX8/9+7rnn0iabbJJq166dLrvsssVeZsaMGenss89OLVu2TB9//HFVxFyhLHwumDlzZrnf7ZRSev755xcpub/88st0/PHHK1CW0E/L7fvvvz/lcrmUy+XSM888k8rKytLjjz+ett5667TrrrumiRMnphdffDH95S9/SY0bN07//Oc/C5x+xTd58uRUq1atdMcddxQ6SmYsWLAg/zv9vx/29urVK9WpUyd17dq13PYzzjgjbbvttunzzz+vspxZ9tNy+7PPPkuffPJJ/uerr746bbvttumcc85Jc+bMSSmlNGvWrLT//vunxx57LO22227pmGOOSfPmzfNarxJ++gH7vHnz0vrrr5+GDBlSbp933303bbTRRmnNNddMN910UyFiZl5ZWdkiH96ccsopqW7duun+++9fZH/l9pLbYYcdUp8+fdIXX3yRLrnkktS0adM0aNCg/PkzZsxId999d/r973+fWrdundq3b5/WWGON/Gvs/30+Z/EWPl+89dZbqX///qlNmzapcePGqX///vnXywtNmTIlnXXWWalZs2b5940sP5999lk69thjvSeEAlNws8w8+OCDac0110z77bdfOu+881JpaWkqKytLY8aMSe3atUtvv/12Smnxo4NYvOeffz4dfvjhacGCBelPf/pT2myzzdL06dPT999/nw466KBUq1atdM455+T3nzNnTurSpUvq1q2b4/0LnnrqqbTaaquladOmpZRS+uSTT9IGG2yQNthgg7TbbrvlPyD46ciHJ554Ih155JFp9dVXT++8805BcmfZwhfoI0aMSLvsskvacsst00477ZTef//9VFJSklL6v5L76KOPzr9o97u8ZH5abt97772pVq1a+XLq4YcfTimlNHfu3PTAAw+kDh06pEaNGqWNNtoodejQwUjXKjJz5sx09NFHp1NOOaXQUaq9f//73+UKkBdeeCH169cv9e/fP3344YcppZReffXVtPPOO6dtt902jRgxIj3++OPp1FNPTU2aNEnvvfdeoaJnVv/+/dPGG2+cmjdvno499tj8BwRnn312at++fdpqq61S375909Zbb5222mqrlFJKhxxySPr9739fyNiZ9dBDD6VDDz00TZgwIR122GHpuOOOS19//XW5fY4++ui01lprpQ4dOqTvvvvOhwiV8Nxzz6Wjjjoq7bvvvmnw4MFp1qxZ+fMWltwPPPBAARNm17XXXptWX331/Ou0yZMnp0svvXSRkjulH7+h8MEHH6TDDjssrbvuumnNNddc5PecX/bEE0+kWrVqpfPPPz9dc801ae+9904bb7xxOvXUU/MfPr700kvpgAMOSBtssIHXdVXIKHkoPAU3y8xnn32WXnrppfTtt9+W237hhRemPffcM33xxRcFSpZ9V155ZerQoUPabLPN0korrZT+/e9/58975JFH0o477pi22WabdM8996Qbb7wx7bnnnmnTTTfN/6FVDC7etGnT8r+bC0er/ec//0kjRoxIu+22W9ppp53yJffCkmXEiBHp6quv9in9Unj00UdT48aN05lnnplGjhyZtt9++7T55punJ598Ml9yv/DCCymXy6UTTzyxwGmz4affREjpx9Ikl8vlR/zttttuafjw4Ytc7s0330yfffaZEWtVbNy4cWmvvfZK8+bNK3SUauu2225LG2+8cXr88cdTSj9OYVSrVq203377pebNm6ctttgi/f3vf08p/fih2GGHHZbq16+fNttss7Trrrsqt5fQTz9AGD58eFp77bXTbbfdlm6++ea08sorp7333jtNmDAhpfTj37/jjjsu/e53v0vHHnts/qvxhx12WOrdu3eaP3++8vUXLBwAktKPrzd+85vf5J+nb7zxxrTSSiulq666Kn311Vf5yxx//PHpoosuUghW0sMPP5yaNm2aunfvns4///xUr169dNxxx5V7/da3b9+Uy+XSI488UsCk2XTRRRel9u3b5wfgXH311emLL75IF198cWratGk677zz8vsufE+ycPDTDjvskK666qr8NipWVlaW5s6dm/bee+9yH46XlJSkAQMGpLZt26b+/funefPmpVmzZqUHH3wwTZo0qWB5AQpBwc1Smzx5cn706/8aN25catSoka9h/wqHHnpoeuKJJ/I/77///imXy6Xf//73aebMmeX2feyxx1KPHj1Ss2bN0k477ZT+8Ic/5EsTX/1bMhMmTEj169cvNy3Jgw8+mHbbbbe0yy675F8kXnXVVelvf/tbfpQElfef//wnbbXVVumKK65IKaX01VdfpXXXXTe1bNkytWzZMj355JP50uSll14yLckSGDlyZGrcuHH66quv8lM5nHTSSeXmF+3UqVP+TdHCD708PxTWT0cSsqivvvoqbbvttmmHHXZIjz/+eDruuOPSDTfckFL68Xf3wAMPTO3bt0+33357/nd64sSJaebMmYv8nWRRCz9MXOiFF15IF154Ybrtttvy2z744IO01lprpd/97nflnosXPnfMnj07/eUvf0krrbSS5+pf8Omnn5b7eeTIkemyyy5LRx99dLnngnPPPTetvPLK6cgjj0xnnHFGOuaYY9Iqq6yirKqk9957L6233nr5tWlmz56dVl555VRUVJT222+//Ic2KaU0YMAAv7+/wpQpU1LTpk3TZpttloqKitLYsWNTSj9OK/dzJffC55zS0tJ08MEHpyOPPLIgubNkYfm/8Js0++67b+rZs2e5fRYsWJD22GOPtPLKK6cTTzwx/xoaoKYpKvQil2Tbo48+Gocffng88MADMXv27Pz2srKyiIgYMWJEdO7cOQ444IBCRcykSZMmxSabbBJ77rlnflvHjh1jwIABMWPGjOjdu3d88cUX+fO6dOkSw4cPj48//jhefPHFuP3226NOnTqxYMGCqFWrViHuQuY0atQoTjnllLjgggviyiuvjIiIAw44IE466aQoLi6OnXfeOXr27BmnnHJKbLfddlGvXr0CJ86u+fPnx8EHHxzHHHNMfPHFF9GhQ4fYa6+9Ytq0abHOOuvEgAED4umnn4558+bFTjvtFBtuuGGhI1d722+/ffz73/+OVVZZJT799NPI5XJx1VVXxeGHHx6lpaUREbHqqqvGjBkzIiKiqKgoTj/99DjqqKPyz9dUvQYNGhQ6QrU1f/78WGWVVWLEiBGRUopLL700xo0bFxtvvHFERNSqVStuv/32WG+99eLKK6+M22+/PebMmRPrrbdeNG7cOBo3blzge1C9HXnkkfH8889HxI+v2aZMmRK77757nHnmmfHll19GRERKKTbaaKN45pln4v33349+/frFm2++GRE/Hv/JkydHnz594oknnojnn3/ec3UFzjvvvDj77LNj7ty5+W333ntvnH766fHSSy/FzJkz89vPOeecuOyyy6K4uDiefPLJmDJlSjz33HPRunXrAiTPppRSfPPNN3HkkUfG8ccfH5999llsvPHG0b1793j99dfjmWeeiSFDhsT48eMjIuLCCy/0+1tJpaWlsdZaa8Xuu+8e48aNiz333DPatGkTKaVo0aJF9OzZM84888y47LLL4sILL4yIiOLi4kgpRVFRUbRo0SK++OKLKCkpiZRSge9N9ZXL5eKBBx6Igw8+ON5///1Ya621YtKkSfHFF1/kj1utWrVi1113jZVWWik+//zzmD59emFDAxRKAct1Mu6RRx5J9erVS0OHDl1kVEpKP34Vbcstt0xnnnlmAdKtOK655pp011135X++4oorUseOHVP37t3LLZz15ptvlvvE3tf9Km/q1Klp4MCBqXHjxmno0KH57S+//HL685//nLp165bef//9AiZccSwcOXXiiSemAw88MH3//fcppZSOPPLIlMvl0m9+85tFFp/kl02cODHlcrl08cUX57ctHNl6/vnnp3322Sel9OPie3Xr1k1vvPFGQXLCz/npdFo//Rs2c+bMtMcee6RcLpeuvvrqcpeZM2dOOuyww9IGG2zg22KVcOqpp+a/6bXwv2+88UZq3Lhx+t3vfpcmT56cUvq/x2H8+PGpdu3a6c9//nO563n33XfTZ599VoXJs+m1117LL/Q2ffr0/PYzzjgj5XK5dM011yzyjY7S0tI0d+5c3/SopIXHa/r06emf//xnWrBgQTrggANSjx490pw5c1JpaWnaaqutUi6XS0cccYRpopbCN998k0455ZT097//PTVt2jR169at3JSUX375Zbr00ktTLpcrN0Xa2LFjU/v27c0PXYGFz73ffvtt6tixY346lylTpqTmzZunQw89tNxzb58+fdL5559fbmojgJpGwc2v8vnnn6ctt9wy/0Zz7ty56dtvv00PPfRQvgD88ssvU9++ffMvHBWulffVV1+lww8/PK2//vrlvjI8dOjQtNNOO6Vu3bqlMWPGpE6dOqXddtutgElXHIsruVNK3gT9Cgv/v580aVL69NNP09SpU/PnlZaWpn322Sf169cvv+3UU09VmCyF+fPnp3PPPTcVFxfn3wwtdPnll6cdd9wxDRo0KBUXF+cX/oXq5OOPP85PQ3LvvfemfffdN82fPz999913aeedd05bb711GjFiRLkyfM6cOalnz5759RJYvP9dk+P6669PN954Y35Kl1deeSUVFxen7t275wcvLHwenzx5cn5qEq/pltxPj9WLL76YunXrlv7xj3/kt5144ompXr166Y477ig3/Zn1UyrvySefTN26dUvjxo3Lb/v+++/Tdtttl26++eaU0o/HtU+fPunpp5/OL1TLr7dwcM3o0aNTkyZNFim5p06dmu68885FFuCz7scve+aZZ1LPnj3TQQcdVG4w2dtvv51WXnnl1LFjx7T//vunQw89NNWrV8/aQECNp+Cm0srKytJ3332X2rVrl2655ZZUUlKSzjnnnNSxY8fUsmXLVLdu3fT888+nlJJyu5J+7s3Mu+++m0488cTUtm3bciX3ddddl3bccce0+uqrp44dOy4ylya/3sKSe6WVVkpDhgwpdJzMe/DBB9Paa6+dVl999dSlS5d0//3358/r0qVL2mijjdItt9ySTjjhhNS0adP8Yp/8soXPre+880569dVX07x589L8+fPTxRdfnHK5XLmS+7bbbku5XC6ttNJKacyYMYWKDIu1YMGCNGTIkJTL5dIxxxyzyKi/b775JnXs2DF17NhxkZKbX2f33XdPG264Ybrzzjvz36R56aWXUnFxcerRo8fPftho7v5fb9SoUWmVVVZJhx12WLlv0Bx//PGpXr166c4770yzZ88uYMLseuihh1KjRo3SGWeckd5555389q+++iqtuuqq6aSTTkpvvvlmGjBgQFprrbXSd999V7iwK5iFz8VvvfVWatKkSTrkkEPKDWhYaP78+Z63K+Gxxx5LuVwu1a9fP/3zn/9MKf3fsf70009Tv3790iGHHJL+8Ic/5M8HqMkU3FTK8OHD09ChQ9N3332XjjjiiLTlllumJk2apP322y8NHTo0ff7552n33XdPf/zjHwsdNXN++oLviy++KPfCcPz48enYY49dpOT+5JNP0ttvv52/7P+OjuDXmzp1avrzn/+c1lprrfTtt9/6kOZX+vTTT9OGG26YbrzxxnTbbbelHj16pI033jjdeOONKaUfR17usMMOaeONN06bb765r6tWwsLfyYceeii1bNkyXXTRRfkPB2bPnp0uuuiiciX3rFmz0s4775xfBAqqo9mzZ6cDDjgg5XK51L179/z2haMEv/7669SxY8e08847p4cffthzcyUsrlg68MAD06abbpruuOOOfMn98ssvp/r166cuXbqkL7/8sipjrjDKysryHwZ8/fXXacaMGSmlHxdgX2+99dLBBx9cruQ+6aSTUi6XS/fee29B8mbZJ598kn7zm9+kyy+/vNz2hb/zI0aMSHXq1EnrrbdeWmONNcoV4CwbC4/1mDFjUvPmzVOnTp2M0l4GnnvuudSgQYPUq1ev/Idf//tNGt8wBfiRgpsl9vnnn6d27dqlCy64IKX04wv0Bx54IN100035N0QppdS1a9d07rnnFipm5p111lmpbdu2ad11103bbbddGjVqVErpxzmLjz322LTRRhul22+/fZHLGVG17E2bNs1cdr/CTwunzz77LHXv3j3/4cv48ePTSSedlNq2bZsvuVP68fll4Zt/ltwzzzyTGjVqlK6//vpyz8MLXXjhham4uDidf/75BUgHlTdv3rx0zDHHpH322Sc1adKk3LcQflpyt2vXLnXq1Mlc/Uvop+X2+++/nz777LP07bff5rd17dp1kZL7ueeeSzvvvLMRl5U0YsSIch8kPvjgg2nbbbdN6623XurSpUt66qmn0sSJE3+25O7bt28aP358IWJn2tixY1ObNm3SRx99lFL68XXI/374NXny5DR27Nhy02ewbC18rnjttddSp06dPHdUwsLf1x9++GGRefcff/zxVLdu3dS7d+9y6y0tPL4+6AX4US4lyxZTsbKysigqKooXX3wx+vXrF1deeWV06NBhkf2++eabuPzyy+PGG2+MV155Jdq2bVuAtNmz8PhGRNx2221x6qmnxmWXXRYNGzaMW2+9NcaNGxdDhgyJP/zhD/Gvf/0rrr322rj77rvjnnvuiT333LPA6aG8lFLkcrl4+umn4+6774769evHv//973j++efz+3z44YcxbNiweOmll+LYY4+NP/3pTwVMnF2lpaXRq1evqFevXvztb3+LWbNmxaRJk+Luu++OBg0aRPfu3WOttdaKgQMHxrBhw2LChAnRvHnzyOVyhY4OFZo3b17MmTMnrrjiirj88svj/PPPj5NPPjl//pw5c6K0tDS++eabWGeddQqYNHv69+8f9957b3z//fdx8MEHR7du3WLXXXeNiIj9998/Jk6cGP369YsuXbpEkyZN8pf76WsVFm/atGnRoUOH2GWXXeKss86KuXPnxnbbbRf9+vWL2rVrx+TJk+Omm26Km266KXbcccfo1KlTbLvttnHiiSfGDjvsUOj4mfXOO+/EDjvsECNGjMj/Pi98PfLKK69EkyZNYvPNNy9wyprhf58rPHf8soW/q0899VRcfvnlMWPGjFhppZXi6quvjnXXXTdq164djz/+eBx88MFx/PHHx5AhQ6JevXqFjg1Q7Si4WWLbbbddtGnTJu64445FznvooYfiiSeeiJEjR8YjjzwSv/3tbwuQMNseeeSR+O9//xsNGzaMnj175rf37Nkznn322Rg1alT85je/iXfeeSdGjhwZffv2jVq1ahUuMPzEwhfnERHPPfdcdOnSJTp16hTTpk2LMWPGxDXXXBMnnHBCfv+PPvooBg8eHB988EE899xz0aRJE8VrJaWUolevXjF//vw49thj484774wpU6bExIkTo1WrVtG4ceN4+OGHI6UUc+bMiZVWWqnQkaFSpk6dGtdff31cccUVcd5558XJJ58c5557bowdOzbuvvtub/CXwE+fm59//vk46aST4vrrr4+xY8fGU089FRERffv2jb322isiIg488MB49dVX4/bbb4/OnTuXuzxL5p133onjjjsutt1222jWrFmUlJTEJZdcEhERM2fOjNtvvz369u0bTz31VLRs2TJ22mmnOPDAA2PYsGF+p3+lL774Ig466KDYaKON4s9//nO5QTbHHHNMlJaWxg033BB16tQpYEpYvMceeyz+8Ic/xIknnhg777xznHHGGVG3bt244IILYuedd47atWvHE088Eb///e/j9NNPj4svvrjQkQGqHQU3FfrpJ8rnn39+/O1vf4tNNtkkIiJmzJgRX375ZYwfPz5WX331GDNmTOy5556x3nrrFTh19owfPz46duwY06dPj6FDh8bJJ58cc+fOzb/R2WKLLWKrrbaKm266qdzlSktLldxUK//973/j5Zdfjm+++SZ69+4dkyZNihtvvDGGDRsWl1xySRx33HH5fSdMmBCNGzeOVVddtYCJs+2WW26JoUOHxuTJk2OfffaJgw46KA444IC47LLL4tlnn41nnnlGOUWmTZs2LW666aY4++yzo3379jF+/Ph46aWXon379oWOVu3978jJ5557Lp577rl8MfL888/HlVdeGXPnzo3TTjstX3IPGDAgzj//fK8vlsI777wTJ5xwQkybNi323XffGDZsWP68GTNmRJ8+fWLu3Llx9913x2uvvRYtW7aMDTbYoICJs+/WW2+NCy+8MDp27BgHHXRQtGzZMu6+++647bbb4pVXXsm/f4HqZuLEiXHwwQdHz5494+STT45vv/02ttxyy5g5c2Y0atQohg8fHjvssEMUFxfH008/Ha1bt44NN9yw0LEBqh0FN0ukZ8+eMWPGjLjvvvuiTp068cILL8TVV18d48ePj1atWsXzzz8fuVwuateuXeiomfC/bzrnzJkTDz/8cAwcODDWWWed/HQO8+fPjzp16sQf/vCHKC4ujltuuaVQkeEXTZo0KdZff/1o0aJFXHDBBfHHP/4xIiI+//zzuPbaa+Oqq66Kyy+/PL+dZeOjjz6KWbNmxZZbbpl/bunbt2989NFHcd9990XDhg0LHRGWypw5c+Ktt96Kt956K7p27Rrrr79+oSNlytChQ2Ps2LHxww8/xDrrrBOXXXZZ/ryRI0fGlVdeGSUlJXHCCSdE165d8+f5EH3p/POf/4z99tsv6tWrF3fffXdsscUW+fPOPPPMeOKJJ2L06NFGbS+ln37L4K677oo77rgjRo0aFa1bt446derE7bffXu7YQ3Xz0UcfxcMPPxynnHJKTJ8+PXbcccfo3LlzXHnllfHb3/42GjRoEIMGDYpOnTp5rw1QARNi8YteeumlePbZZ+Oiiy6Khx9+OI4++ujo0qVLrLPOOnHxxRfHSy+9FHXq1PEHdwn9tNy+5ZZb4pFHHomIiAMOOCDOP//8eO+99+KAAw6IiMi/YP/444+jfv36BckLS2rNNdeMiy66KGbPnh0TJkzIb1999dXjpJNOilNPPTWOPfbYGD58eOFCrkAWfj7dtm3b2HLLLSPix/nN+/XrFzfffHMMGTJEuc0KoX79+rHTTjvFaaedptxeAmVlZfl///Wvf41BgwbFzJkzY+zYsXHDDTfEiBEj8ufvvvvu0adPn/j+++9j1KhREfF/zy3K7aWz2WabxWOPPRZ16tSJK6+8Mt577738eV9//XW0bNkySktLC5hwxZDL5fK/84cffnjcf//98a9//SueeOKJGDVqlHKbamvs2LExbdq0aNu2bey///5Rv379GDRoUGy55ZZx8cUXR+3atWOTTTaJt956K/r16xfz5s0rdGSAak0jyS8aNWpUlJSUxBFHHBFTp06NXr16xTPPPFNuMRxzNC6ZlFK+3O7Xr1/cdtttcd5558UPP/wQLVq0iN///vcREXHiiSdGu3btYt11143GjRvHjBkz4sorryxkdFjEwv/v33333Zg9e3Zss802cfLJJ0dRUVH8+c9/jjXXXDO/gORqq60WxxxzTBQXF//sIrVU3v8+544dOzbOPPPMmDZtWrz88svRrl27AiUDCmnh64wPPvggiouLY8SIEdGxY8cYM2ZMXHHFFdG/f//I5XKx9957R0TEbrvtFldffXV+/RSv55addu3axW233Rbdu3ePAw44IHbaaaeoW7duPPjgg/H888/7EHIZKSoqyr8madiwoekSqdZSSjF37tzo1KlTHHbYYXHVVVdF27Zto6ysLD755JPYZptt8s8Nq6++erzzzjux8sorR4MGDQqcHKB6M0UJFVqwYEGccMIJMX78+Nhhhx2if//+0bRp08jlckrtpTB06NAYPHhwPPPMM/mRJT/9KvD9998fAwcOjDlz5sRjjz2WL6oWLFhgpDzVwsL//x9++OE4/vjj47TTTotDDz001l577Zg9e3YMGzYs+vfvH1deeWW+5I7wlffl7a233oo11lgjVl999UJHAQro6aefjr333jvWXHPNePzxx2PzzTePiB+fI6666qoYO3ZsXHLJJfl5txf63ynUWDbGjRsXBxxwQJSUlMSJJ54Yhx12WKyzzjqFjgVUof997/zAAw/EySefHH/7299i3333jYiIzp07x1dffRWnn356vP766/H3v/89xo0bF2uuuWahYgNkhoKbXzRjxoxIKeWLbW9+lk5KKf74xz/GKqusEhdddFFMmjQpxowZE1dddVW0adMmunXrFnvttVfce++90b9//9h2223jnnvuiQjlINXLs88+GwceeGBceumlccQRR0SjRo3KnT948OAYNGhQnHfeefGXv/ylQClrBh84Aj/14YcfxrXXXhs33HBD/P3vf4+DDz44f96YMWNi2LBh8eSTT8Zjjz0W2223XQGT1hxvv/12DBgwIO68885o0aJFoeMABfDmm29Gy5YtY7XVVouysrI46aSTIiLivPPOizXWWCO+++676NSpU8yaNStq1aoVf//7302zA7CEFNxUihKl8n56zFJKMX/+/DjggANi9uzZ0bVr13jssceiuLg4mjRpEjNnzoxatWrFQw89FCUlJTFixIg444wzom3btvHkk08W+J7A/yktLY1evXpFvXr14m9/+1vMmjUrJk2aFHfffXc0aNAgunfvHmuttVYMHDgwhg0bFv/+97+jefPmhY4NsMJZ3MCDCRMmxJAhQ+Kee+6J++67L/bZZ5/8ea+99lqMHDkyzjjjDB+cV6G5c+daVBJqqMmTJ8cGG2wQG2ywQRx55JHRo0ePmDp1ahxwwAFxzTXXRJcuXfL7fv7559GwYcNo2rRpARMDZIuCG5aj/33TOXv27GjQoEF8/PHH0aNHj/jmm2+iV69esfvuu8c222wTV199dTz66KPx5JNPRnFxccyZMyfuv//+uOSSS+Lpp5+ONdZYo4D3Bv5PSil69eoV8+fPj2OPPTbuvPPOmDJlSkycODFatWoVjRs3jocffjhSSjFnzpxYaaWVCh0ZYIXz0w/Rb7nllvjqq6+ibt260adPn4iImDRpUgwePDjuu+++uOuuu/Lzbv+Ub4cBLH9fffVV9OrVK8aPHx89evSIv//973HXXXfFrbfeGk888US8++67Xi8DLAXzTMBy8tNy+7LLLovDDz88ttxyy7jooouiXr168eqrr8Ybb7wRAwYMiG222SZKS0vj6aefjtVXXz3q1KkTERH169ePbt26xWuvvabcplrJ5XKx0047xbhx46JLly7x/fffxzHHHBMff/xxdO3aNUpLS6Nu3bpRv359L9YBlpOF5fbAgQOjT58+8cwzz8RZZ50Ve+21V3zzzTex7rrrxoABA+KQQw6J7t27x4MPPrjIdSi3AZafSZMmxZdffhktWrSIv/71r/H111/H2muvHQMHDoyDDjooSkpKYurUqfHXv/415s2bV+i4AJllBDcsZwMGDIhbb701+vXrFw0bNox+/frF7rvvHtddd120aNEiZs6cGSNGjIg777wzPvnkk3jnnXeiTp065jonEz766KOYNWtWbLnllvnf2b59+8ZHH30U9913X34VeACWnZ++Rpg3b1706NEj+vTpE5tvvnlMnDgx9tlnn2jdunXcf//90aJFi5g0aVL069cvZsyYEc8880yB0wPUDP/5z3+ie/fuUb9+/bjkkktiiy22iOuvvz5GjRoVw4cPj3HjxsX9998fQ4cOjfXXXz9Gjx4dTZo0KXRsgEyqXegAsCIbM2ZMPPTQQ/HII4/EdtttF2PGjInvv/8+9ttvv/wCQ7NmzYoHHngg6tevH++++27Url07FixYELVr+9+T6mvh1+Lbtm2b3/bhhx/GbbfdFjfffHO8+uqrym2A5eCn5fZHH30UM2bMiAYNGsRqq60W9erVi0022SSee+656NSpU3Tr1i3uv//+WHfddePKK6+MVq1aFTg9QM3w7rvvxnrrrRd/+tOf4sknn4xtt902Lr744mjWrFk0atQoRowYEQceeGC0adMmDj300GjatKlyG2ApGMENy9Ho0aOjd+/e8dZbb8V9990XRx99dFx88cVxwgknxA8//BBvvPFG7LHHHvHNN99E8+bNo6ioyFyYZNLYsWPjzDPPjGnTpsXNN98cm2/+/9q78+Cq6vOP4++bkBBDAilgwoUIyh5pkYI4aqVVQILCsI2iuLFIZUfRKrUgBAQFKVukoCwCTmSxGGOhGlExGqNolaKgBBVQZAswiIgRkpD8/nC4P1NtVSSGwPs1c2dyz3LPczKTMzef8z3P94LyLkmSTmt33303S5YsITw8nP379/P000/ToUOHUNuSjz76iKuuuirUFi0uLg7475NSSpJOzPHBScevr5s3b6Zdu3asXbuWxMREAP72t78xf/58mjRpwtq1a6lTpw6ZmZnExsaWc/WSdHrw2610kuzZs4cNGzaQlpbGxo0b+fzzz6latSo7d+5k7ty53HbbbUyePJlBgwYBsHbtWmbPnk1ubi41atQgLCyM4uJiw21VSC1atCAlJYV//OMfhtuSVAaKi4tDP69cuZLnnnuO6dOn8/DDD1OnTh3uu+8+Nm7cGNqmUaNGrFy5kqZNm5YKUAy3JenkmTt3LklJSXz99deh62tUVBTR0dFUrlyZoqIiAIYMGcLs2bP59a9/TUxMDG+88QYpKSnlWLkknV4cwS2dBOnp6SxYsIB169aRn59PYWEhV155JaNGjWLZsmXMmDGDsWPHMnbsWACOHj3KNddcQ2RkJH//+9/9Z1MV2vF2JZKkspeRkcFrr73G2WefzciRIwH4/PPPadmyJQkJCcydO5fmzZt/Zz+fEJOkk++dd97h+uuvp2bNmrz00ktER0fz1ltvcfPNN7NhwwYiIyNLfVfOz89n27Zt3HnnncyYMYOkpKRyPgNJOj0YcEs/07x58xg5ciSjRo2iRYsWtGrViocffpglS5ZQUlJC79692bhxI6+//jrjxo3j888/59lnn2Xnzp38+9//dkJJSZL0Xx0PRoqLi8nPz6dly5Z8/PHH3HzzzSxevDi03cGDB2nVqhW1atUiNTWVVq1alWPVknTmeO+997juuuuIiYnh1Vdf5YMPPuD666/n/fffJzIy8nv3cYCIJJ1cBtzSzzBv3jyGDh3K0qVL6dGjR6l1y5cvZ8qUKVSpUoVBgwaRnZ3NypUradiwIfXr1+eRRx5xQklJkvSjHDhwgOrVq5OXl8f111/Pvn37mDx5MldddVXoJvnBgwepXbs2N910E3Pnzi3niiXp9PbtkPq9996jZ8+exMfHM3HiREaPHk2PHj1o1KgRsbGxHDp0iIMHD3LhhRfSpEkTA25JOskMuKUTlJWVRdu2bUlJSWHMmDEc/1M6duxYKLBOTU1lzJgxPPbYY/To0YN9+/Zx9tlnhz7DcFuSJP2QxYsXs2rVKsaPH09SUhJ79uyha9euREVFce+995KcnBwKSr766iuioqJsRyJJZeh4QP3RRx8RFhZGgwYNeO+99+jVqxebNm2iadOmVK1alYMHDxIREUF+fj6VK1cmIyODxo0bl3f5knTasSeCdILq1KnDZZddxrp168jOziYQCBAIBEIzaAMMHz6cc845hxdffBGAuLi40P4lJSWG25Ik6QcdOHCATz/9lGnTppGbm0utWrXIyMjgyJEjTJo0iRdeeCF0o71KlSqEh4dz7Nixcq5akk5Px8Pt9PR0unXrRnp6Onv37qV58+akpaXRpk0bCgsLWb16Nbm5uaxdu5bc3FzefPNNw21JKiMG3NIJatSoEQsWLODo0aNMnDiR1157LbTu+CiqQ4cOceTIEYLBIAARERHf2UaSJOm44zfJv23EiBHceuutbNy4kSlTppCbm0swGCQjI4OCggJGjBjBW2+9VWofR3BLUtkIBAKsXr2am266icGDB9OnTx/i4+MBaNGiBTNnziQsLIxOnTpx+PBhqlSpQkREBLGxseVcuSSdvgy4pZ+hUaNGpKamEggEmDBhAjk5OaXWb926lcTERC6++GIA7AgkSZL+l+P9tLOysti9e3do+YABA+jduzebNm1iypQpfPzxxwSDQZ588kkuvfRSLrzwwvIqWZLOGMXFxRw9epQFCxYwePBghgwZEmpBeezYMQKBAC1atGDFihV8+OGHdO3atZwrlqQzgwG39DN9O+S+//77Q+1KioqKGDVqFDExMbRr1w5w1LYkSfph2dnZ9O3bl5kzZ5KXlxdaPnDgQK699lpWrFjB5MmT2bBhA4mJicybN8+2JJL0CwgLC6Ny5cps376ds846C/j/QUzHn5zZsWMHv/nNb1izZo0T/krSL8SAWzoJvh1yT5o0iZycHK677jo++eQT0tPTCQsL+95HjiVJkv7zCa82bdrQq1cv1qxZ852Qe9iwYQSDQTIzM1m1alWp/W1LIkllq6SkhMLCQsLDw9m6dSvwzSCm49fhHTt2sHjxYrZv306zZs1o0KBBeZYrSWcMA27pJPl2yH3FFVfw/vvvs379eiIiIigqKgo9cixJknRcQUFB6Amvr7/+mvz8fAAeeOABOnToQGZmJjNnzmTfvn0A7Nq1i8suu4yxY8cycuRIwCfEJKms/OcNyEAgQEREBPfeey/Lly/ngQceCC0HePjhh8nIyAiN7pYk/TICJTYFlk6q3NxcZs+ezbRp06hUqRJFRUVUqlSpvMuSJEmnkGeffZarr7469P7BBx/k+eefp2rVqnTo0IGhQ4cCMHr0aFavXk3dunW5+uqrefLJJ6lcuTIZGRkEAgGOHTvmyG1JKgMlJSUEAgGysrJ49dVX2bJlC/379+f888+nRo0aTJkyhb/85S907tyZhIQEvvrqK1auXElWVhYtWrQo7/Il6YzikFLpJGvatCmpqamG25Ik6XstX76czp07M2vWLACmTp3KtGnTuPTSS4mJiWH06NGh0dkTJkygT58+5OfnM3nyZMLDw1mxYkXokXjDbUkqG4FAgKeffpouXbqQm5vLrl27GDhwINOmTSMvL4+7776bl156iZKSEnbt2kVkZCSvv/664bYklQNHcEuSJEm/oF27djFv3jymT5/O+PHjCQ8Pp2HDhiQnJ/PFF1+wbNkyhg4dyogRI3jooYcA+PLLL8nPzyc+Pj40mbU30SWp7Lz55ptce+21pKSk0K9fPw4fPkzNmjWpXbs23bt3Z8SIESQmJoaux16XJan8ePWVJEmSfkG1a9dm4MCBAKSkpBAeHk56ejoA1apV46abbgK+mVAyPDycBx98kNjYWGJjYwEoLi42RJGkMrZr1y569OhBv3792LZtG+3ataNv374kJCQwZcoUwsPDGThwIPXr1wec6FeSypPfjCVJkqRfWEJCArfddhsRERGMHz+enJwc2rRpA0CVKlW4+eabCQsLY8CAAZx77rkMGDAgtK8TV0tS2bv44otp1qwZBQUFDBkyhCuuuII5c+YAkJaWRlpaGpGRkaSkpFCpUiUn/JWkcmTALUmSJJWDYDBIv379OHr0KOPHj6dKlSoMGzYMgOjoaHr16kV8fDydOnUq50ol6fRzfBLJoqIiwsPDCQQCFBcXh24iBoNBgsEgn3zyCdu3b2fw4MEA7Nmzh5YtW9KgQQP++Mc/+kSNJJ0CvBJLkiRJ5aRWrVoMHjyYQCDA6NGjCQQCDB06FICYmBi6du0KYG9XSTrJ1q1bR6tWrULX1szMTBYsWMDRo0dp3rw5gwYNonbt2hQVFVFUVMTmzZtJSkoiLS2NPXv2MH/+fKpWrVrOZyFJAvD5RkmSJKkcJSQkMGjQIO68807GjBnDpEmTvrON4bYknTzZ2dm0bt2amTNnAvDCCy/QqVMnzjrrLGrVqsWjjz7KDTfcQE5ODg0bNqRjx46kpqbStm1b5syZw4wZMwy3JekUEigpKSkp7yIkSZKkM11eXh4PPfQQGzZs4Pnnn7efqySVkd27dzNnzhxmzZpFSkoK1apV48CBA4wYMQKAffv2kZycTHR0NKtXryY6OpoXX3yRkpISGjduTL169cr5DCRJ32bALUmSJJ0iDhw4wK9+9SsCgUCoP6wk6eTLy8tjzpw5pKamEhERwX333cfQoUMpKCggMjKSvXv3kpSUxO23386YMWPKu1xJ0v9gixJJkiTpFFG9enXDbUn6BSQkJDBgwAD+9Kc/kZ+fz9atWwGIjIyksLCQ+Ph42rdvH1ouSTp12cxPkiRJOsUYbktS2QsGg/Tt25eCggLuv/9+6tatyx133EFERAQAX3zxBVWrVvWmoySd4gy4JUmSJEnSGSkYDDJ48GBKSkq466672Lx5M4mJiRw6dIhXXnmFt99+23Bbkk5x9uCWJEmSJElntL179zJ79mxmzZpFZGQkqampnH/++Zx//vnlXZok6QcYcEuSJEmSpDPe7t27mTVrFkuWLOGdd96hevXq5V2SJOlHMOCWJEmSJEkC8vLyCA8Pp2bNmuVdiiTpRzLgliRJkiRJkiRVSGHlXYAkSZIkSZIkSSfCgFuSJEmSJEmSVCEZcEuSJEmSJEmSKiQDbkmSJEmSJElShWTALUmSJEmSJEmqkAy4JUmSJEmSJEkVkgG3JEmSJEmSJKlCMuCWJEmSJEmSJFVIBtySJEmSJEmSpArJgFuSJEmSJEmSVCEZcEuSJEnA5Zdfzh133FHeZUiSJEn6CQy4JUmSVCb69OlDt27dSi1bsWIFUVFRTJ06tUyOFwgE/uvr3HPPPenHlCRJklS+DLglSZL0i5g/fz433ngjc+bM4a677jrpnz9z5kx2794degEsXLgw9P5f//rXST+mJEmSpPJlwC1JkqQy99BDDzFs2DCWLVtG3759Q8ufeeYZWrZsSVRUFPXr12fcuHEUFRUB0K9fPzp37lzqcwoLC4mPj2fBggXfOUa1atWoVatW6AUQFxcXev/BBx9w0UUXUblyZYLBIH/+859Dx/o+//znP6lWrRpPPPEEAJ999hk9e/YkLi6O6tWr07VrVz755JPQ9sdHrP/1r38lGAxSo0YNhgwZQmFhYWib2bNn06hRI6KiokhISOCaa6756b9MSZIkSSEG3JIkSSpTI0eO5P7772fVqlV07949tDw7O5tbbrmF22+/nQ8++IBHH32URYsWMXHiRAD69+9PZmZmaDQ2wKpVq8jPz+e66677STXs3LmTq6++mtatW/Puu+8yZ84cFixYwIQJE753+yVLltCrVy+eeOIJbrzxRgoLC0lOTiY2Npbs7GxycnKIiYmhY8eOFBQUhPZ7+eWX2bJlCy+//DKLFy9m0aJFLFq0CIC3336b4cOHM378eDZv3kxmZia///3vf9J5SJIkSSotUFJSUlLeRUiSJOn006dPH5YuXUpBQQEvvfQSbdu2LbW+ffv2tGvXjnvvvTe0LC0tjXvuuYddu3YB0KxZM3r37s0999wDQJcuXahRowYLFy78weMHAgGefvppunXrxqhRo3jqqafYtGkTgUAA+GY09ciRI/niiy8ICwvj8ssvp0WLFjRq1IhRo0bxzDPP8Ic//CFU14QJE0rtX1BQQFxcHBkZGXTo0IE+ffqQlZXFli1bCA8PB6Bnz56EhYWxbNky0tPT6du3Lzt27CA2NvZn/nYlSZIkAVQq7wIkSZJ0+mrevDn79+9n7NixXHTRRcTExITWvfvuu+Tk5IRGbAMcO3aMI0eOkJ+fT3R0NP3792fu3Lncc8895OXl8dxzz7FmzZqfXMemTZu45JJLQuE0wO9+9zsOHz7Mjh07qFu3LvDNJJh79+4lJyeH1q1bl6r1448//k4wfeTIEbZs2RJ636xZs1C4DRAMBtmwYQMAV155JfXq1aN+/fp07NiRjh070r17d6Kjo3/y+UiSJEn6hi1KJEmSVGbq1KlDVlYWO3fupGPHjnz55ZehdYcPH2bcuHGsX78+9NqwYQMfffQRUVFRANxyyy1s3bqVN954g7S0NM477zzatGlTZvX+9re/5eyzz+axxx7j2w86Hj58mFatWpWqdf369Xz44YfccMMNoe0iIiJKfV4gEKC4uBiA2NhY1q1bx9KlSwkGg4wZM4YLLriAgwcPltn5SJIkSac7R3BLkiSpTNWrV49XXnmFK664go4dO5KZmUlsbCwtW7Zk8+bNNGzY8L/uW6NGDbp168bChQt54403Sk1Q+VMkJSXx1FNPUVJSEhrFnZOTQ2xsLImJiaHtGjRowNSpU7n88ssJDw9n1qxZALRs2ZLly5cTHx9P1apVT6gGgEqVKtG+fXvat2/P2LFjiYuLY82aNfTo0eOEP1OSJEk6kzmCW5IkSWXunHPOISsri71795KcnMyhQ4cYM2YMjz/+OOPGjeP9999n06ZNLFu2jNGjR5fat3///ixevJhNmzbRu3fvEzr+4MGD+eyzzxg2bBi5ubk888wzjB07ljvvvJOwsNJfiRs3bszLL7/MU089xR133AHAjTfeSM2aNenatSvZ2dls27aNrKwshg8fzo4dO35UDatWrSI1NZX169fz6aef8vjjj1NcXEyTJk1O6JwkSZIkGXBLkiTpF5KYmEhWVhb79+8nOTmZSy65hFWrVrF69Wpat27NxRdfzPTp06lXr16p/dq3b08wGCQ5OZnatWuf0LHr1KnDs88+y1tvvcUFF1zAwIEDufXWW78Tph/XpEkT1qxZw9KlS7nrrruIjo7m1VdfpW7duvTo0YOkpCRuvfVWjhw58qNHdMfFxZGenk7btm1JSkrikUceYenSpTRr1uyEzkmSJEkSBEq+3VxQkiRJOsUcPnyYOnXqsHDhQlt5SJIkSSrFHtySJEk6JRUXF7N//36mTp1KXFwcXbp0Ke+SJEmSJJ1iDLglSZJ0Stq+fTvnnXceiYmJLFq0iEqV/OoqSZIkqTRblEiSJEmSJEmSKiQnmZQkSZIkSZIkVUgG3JIkSZIkSZKkCsmAW5IkSZIkSZJUIRlwS5IkSZIkSZIqJANuSZIkSZIkSVKFZMAtSZIkSZIkSaqQDLglSZIkSZIkSRWSAbckSZIkSZIkqUIy4JYkSZIkSZIkVUj/B8NhBNCDt2MtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1600x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_attention(model, tokenizer, input_text, layer=0, head=0):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "    # Get attention weights for specified layer and head\n",
    "    attention_weights = attentions[layer][0][head].cpu().numpy()\n",
    "\n",
    "    # Get correct token labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # Process tokens for display\n",
    "    display_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == '<|begin_of_text|>':\n",
    "            display_tokens.append('<s>')\n",
    "        elif token == '<|end_of_text|>':\n",
    "            display_tokens.append('</s>')\n",
    "        elif token.startswith('Ġ'):\n",
    "            display_tokens.append(' ' + token[1:])  # Replace 'Ġ' with space for clarity\n",
    "        else:\n",
    "            display_tokens.append(token)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(attention_weights, annot=False, cmap=\"YlOrRd\", xticklabels=display_tokens, yticklabels=display_tokens)\n",
    "    \n",
    "    plt.title(f\"Attention Heatmap (Layer {layer+1}, Head {head+1})\\nInput: '{input_text}'\")\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Despite its small size, the hummingbird can fly backwards.\"\n",
    "visualize_attention(model, tokenizer, input_text, layer=0, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attention heatmap for the sentence \"Despite its small size, the hummingbird can fly backwards.\" reveals interesting patterns in how the model processes information in its first layer and first attention head. The start token '\\<s>' shows strong attention across all tokens, suggesting its role in gathering context for the entire sentence. There's a notable pattern of local attention, where tokens attend strongly to their immediate neighbors, as seen in the diagonal pattern. The word \"Despite\" shows moderate attention to \"small\" and \"size,\" indicating the model's awareness of the contrasting relationship it introduces. \"Hummingbird\" exhibits attention to \"small\" and \"size,\" suggesting the model connects the bird's characteristics. Interestingly, \"fly\" and \"backwards\" show mutual attention, capturing their semantic relationship as the key action. The token \"can\" attends to both \"hummingbird\" and \"fly,\" linking the subject to its ability. There's also a subtle attention from \"backwards\" to \"Despite,\" potentially capturing the unexpected nature of the hummingbird's ability despite its size. This layer seems to focus on local grammatical structure and immediate semantic relationships, laying groundwork for higher-level understanding in subsequent layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The capital of France is'\n",
      "Predicted next token: ' a'\n"
     ]
    }
   ],
   "source": [
    "def predict_next_token(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    predicted_token_id = torch.argmax(logits[0, -1, :])\n",
    "    predicted_word = tokenizer.decode(predicted_token_id)\n",
    "    \n",
    "    print(f\"Input: '{input_text}'\")\n",
    "    print(f\"Predicted next token: '{predicted_word}'\")\n",
    "\n",
    "predict_next_token(\"The capital of France is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n",
      " LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "model LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens Embedding(128256, 4096)\n",
      "model.layers ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "model.layers.0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.0.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.1.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.2.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.3.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.4.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.5.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.6.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.7.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.8.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.9.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.10.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.11.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.12.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.13.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.14.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.15.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.16.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.17.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.18.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.19.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.20.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.21.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.22.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.23.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.24.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.25.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.26.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.27.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.28.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLU()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.29.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLU()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.30.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLU()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.31.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLU()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.norm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.rotary_emb LlamaRotaryEmbedding()\n",
      "lm_head Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all named parameters to see their names and types\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# You can also use named_modules to explore the submodules\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama 3.1 8B Model Structure Summary\n",
    "1. Overall Architecture\n",
    "\n",
    "    Transformer-based, decoder-only variant\n",
    "    Components: embedding layer, multiple transformer layers, output layer\n",
    "\n",
    "2. Key Components\n",
    "    a. Embedding Layer (model.embed_tokens)\n",
    "\n",
    "    Converts input tokens to dense vector representations\n",
    "    Dimension: 128256 x 4096\n",
    "\n",
    "    b. Transformer Layers (model.layers)\n",
    "\n",
    "    32 identical layers (numbered 0 to 31)\n",
    "    Each layer contains:\n",
    "\n",
    "    Self-Attention mechanism (self_attn)\n",
    "    Multi-Layer Perceptron (mlp)\n",
    "    Layer Normalization (input_layernorm and post_attention_layernorm)\n",
    "\n",
    "\n",
    "\n",
    "    c. Output Layer (lm_head)\n",
    "\n",
    "    Linear layer mapping final hidden states to vocabulary logits\n",
    "    Dimension: 4096 x 128256\n",
    "\n",
    "3. Detailed Layer Components\n",
    "    a. Self-Attention (LlamaSdpaAttention)\n",
    "\n",
    "    Query (q_proj), key (k_proj), and value (v_proj) projections\n",
    "    Rotary positional embeddings (rotary_emb)\n",
    "    Output projection (o_proj)\n",
    "\n",
    "    b. Multi-Layer Perceptron (LlamaMLP)\n",
    "\n",
    "    Gate and up projections expand dimension (4096 to 14336)\n",
    "    SiLU activation function\n",
    "    Down projection reduces dimensions back to 4096\n",
    "\n",
    "    c. Layer Normalization\n",
    "\n",
    "    Uses RMSNorm, a variant of Layer Normalization \n",
    "\n",
    "4. Model Dimensions\n",
    "\n",
    "    Hidden state size: 4096\n",
    "    Intermediate MLP size: 14336\n",
    "    Number of attention heads: 32 (derived from 4096 / 128)\n",
    "\n",
    "5. Activation Function\n",
    "\n",
    "    SiLU (Sigmoid Linear Unit) used in the MLP\n",
    "\n",
    "6. Notable Features\n",
    "\n",
    "    Rotary positional embeddings instead of absolute positional encodings\n",
    "    RMSNorm for layer normalization\n",
    "    No bias terms in linear layers\n",
    "\n",
    "    \n",
    "This architecture enables effective processing of sequential data, capturing complex patterns and relationships in input text. With approximately 8 billion parameters, the model can learn and represent vast amounts of information, facilitating human-like text generation and various language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.0166, -0.0062, -0.0014,  ...,  0.0149, -0.0128, -0.0027],\n",
      "        [-0.0004, -0.0276, -0.0031,  ...,  0.0132,  0.0059, -0.0082],\n",
      "        [ 0.0131,  0.0008,  0.0139,  ..., -0.0016,  0.0061,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0155, -0.0123,  ..., -0.0120,  0.0111, -0.0161],\n",
      "        [-0.0045, -0.0013, -0.0025,  ..., -0.0195, -0.0192,  0.0029],\n",
      "        [-0.0027,  0.0103,  0.0063,  ..., -0.0065,  0.0166, -0.0077]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model to understand its structure\n",
    "print(model)\n",
    "\n",
    "# Access the first transformer block\n",
    "first_block = model.model.layers[0]\n",
    "\n",
    "# Print the first linear layer weights of the first transformer block\n",
    "first_linear_layer_weights = first_block.mlp.gate_proj.weight\n",
    "print(first_linear_layer_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: '<|begin_of_text|>Capital of France is'\n",
      "Generated text: Capital of France is Paris, the city of love, fashion, art, and cuisine. Paris is a city that has been a major hub for centuries, with a rich history and culture that is still evident today. From the iconic Eiff\n"
     ]
    }
   ],
   "source": [
    "# 4. Text Generation\n",
    "def generate_text(input_text, max_length=50):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    attention_mask = inputs['attention_mask']\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    print(f\"Input: '{input_text}'\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "generate_text(\"\"\"<|begin_of_text|>Capital of France is\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Capital of France is'},\n",
       "   {'role': 'assistant', 'content': 'The capital of France is Paris.'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Capital of France is\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", max_new_tokens=50, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\"))\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Text: <|begin_of_text|>Capital of France is\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device='mps')\n",
    "\n",
    "# Access the tokenizer directly\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Example message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Capital of France is\"},\n",
    "]\n",
    "\n",
    "# Simulate what the pipeline does internally\n",
    "input_text = messages[0][\"content\"]\n",
    "tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Decode to see the formatted text\n",
    "formatted_text = tokenizer.decode(tokenized_input['input_ids'][0], skip_special_tokens=False)\n",
    "print(f\"Formatted Text: {formatted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Inputs: {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   9822,    374, 128009, 128006,\n",
      "          78191, 128007,    271]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'prompt_text': <transformers.pipelines.text_generation.Chat object at 0x17feb6d40>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Capital of France is'},\n",
       "   {'role': 'assistant', 'content': 'The capital of France is Paris.'}]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline\n",
    "\n",
    "# Set the device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    "# If you want to inspect the processing within the pipeline:\n",
    "class CustomPipeline(TextGenerationPipeline):\n",
    "    def preprocess(self, inputs, **generate_kwargs):\n",
    "        # Call the original preprocess method\n",
    "        model_inputs = super().preprocess(inputs, **generate_kwargs)\n",
    "        print(f\"Preprocessed Inputs: {model_inputs}\")\n",
    "        return model_inputs\n",
    "\n",
    "# Initialize the custom pipeline\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\").to(device)\n",
    "custom_pipe = CustomPipeline(model=model, tokenizer=tokenizer, device=device, max_new_tokens=50)\n",
    "\n",
    "# Generate and inspect\n",
    "custom_pipe(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.3, Top-p: 0.3\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create incredible technologies, but it came at a terrible cost. The stars themselves began to fade, and the fabric of space-time started to unravel. The people of Earth were faced with a\n",
      "\n",
      "Temperature: 0.3, Top-p: 0.7\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create a new form of energy, one that could sustain entire civilizations. The discovery of this power, known as the \"Stellar Energy,\" marked a new era in human history.\n",
      "As\n",
      "\n",
      "Temperature: 0.3, Top-p: 1.0\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create immense energy, but at a terrible cost. The energy, known as the \"Starlight,\" was found to be a manifestation of the universe's own life force. As humans began\n",
      "\n",
      "Temperature: 0.7, Top-p: 0.3\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create a new form of energy, one that could sustain entire civilizations. The discovery of this power, known as the \"Stellar Energy,\" marked the beginning of a new era for humanity\n",
      "\n",
      "Temperature: 0.7, Top-p: 0.7\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create advanced technology, and it was known as the \"Starlight.\" The people of the world, eager to tap into this power, formed a powerful organization known as the \"Starlight\n",
      "\n",
      "Temperature: 0.7, Top-p: 1.0\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create energy and sustain life. However, as time went on, humanity began to lose touch with the natural world and the secrets it held. A group of scientists and explorers known as\n",
      "\n",
      "Temperature: 1.0, Top-p: 0.3\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create incredible technologies, but it came at a terrible cost. The stars themselves began to fade, and the fabric of space-time started to unravel. The people of Earth were faced with a\n",
      "\n",
      "Temperature: 1.0, Top-p: 0.7\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to create incredible technologies that transformed the world. The discovery of this power led to a new era of scientific advancement and technological innovation. However, it also created a new set of challenges and conflicts\n",
      "\n",
      "Temperature: 1.0, Top-p: 1.0\n",
      "Generated: In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to drive technology, heal the sick, and even bend time and space. The world became known as Nova Terra, the realm of wonder and possibility. It was an era of unparalleled progress and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Inference Parameters Exploration\n",
    "\n",
    "def explore_inference_parameters(input_text, temperatures=[0.3, 0.7, 1.0], top_p_values=[0.3, 0.7, 1.0]):\n",
    "    # Use the updated method to check for MPS availability\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set pad_token to eos_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        for top_p in top_p_values:\n",
    "            output = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=60,\n",
    "                temperature=temp,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = tokenizer.decode(output[0].cpu(), skip_special_tokens=True)\n",
    "            print(f\"Temperature: {temp}, Top-p: {top_p}\")\n",
    "            print(f\"Generated: {generated_text}\\n\")\n",
    "\n",
    "starting_input = \"In a distant future, humanity discovered a hidden power within the stars. This power could be harnessed to\"\n",
    "\n",
    "explore_inference_parameters(starting_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing LLaMA 3.1 8B model layers for input: 'In the distant future, humanity has mastered the art of interstellar travel.'\n",
      "\n",
      "Layer 1:\n",
      "  Mean: 0.0002\n",
      "  Std Dev: 0.0491\n",
      "  Min: -2.5502\n",
      "  Max: 3.2153\n",
      "\n",
      "Layer 2:\n",
      "  Mean: -0.0052\n",
      "  Std Dev: 2.0646\n",
      "  Min: -294.0639\n",
      "  Max: 321.5550\n",
      "\n",
      "Layer 3:\n",
      "  Mean: -0.0053\n",
      "  Std Dev: 2.0651\n",
      "  Min: -294.1276\n",
      "  Max: 321.6336\n",
      "\n",
      "Layer 4:\n",
      "  Mean: -0.0051\n",
      "  Std Dev: 2.0660\n",
      "  Min: -294.2521\n",
      "  Max: 321.7295\n",
      "\n",
      "Layer 5:\n",
      "  Mean: -0.0045\n",
      "  Std Dev: 2.0672\n",
      "  Min: -294.3688\n",
      "  Max: 321.7903\n",
      "\n",
      "Layer 6:\n",
      "  Mean: -0.0047\n",
      "  Std Dev: 2.0685\n",
      "  Min: -294.5399\n",
      "  Max: 321.8500\n",
      "\n",
      "Layer 7:\n",
      "  Mean: -0.0047\n",
      "  Std Dev: 2.0699\n",
      "  Min: -294.6727\n",
      "  Max: 321.9390\n",
      "\n",
      "Layer 8:\n",
      "  Mean: -0.0044\n",
      "  Std Dev: 2.0714\n",
      "  Min: -294.8217\n",
      "  Max: 322.0103\n",
      "\n",
      "Layer 9:\n",
      "  Mean: -0.0043\n",
      "  Std Dev: 2.0726\n",
      "  Min: -294.9132\n",
      "  Max: 322.0692\n",
      "\n",
      "Layer 10:\n",
      "  Mean: -0.0050\n",
      "  Std Dev: 2.0736\n",
      "  Min: -294.9958\n",
      "  Max: 322.1019\n",
      "\n",
      "Layer 11:\n",
      "  Mean: -0.0051\n",
      "  Std Dev: 2.0838\n",
      "  Min: -296.2489\n",
      "  Max: 323.7329\n",
      "\n",
      "Layer 12:\n",
      "  Mean: -0.0048\n",
      "  Std Dev: 2.0845\n",
      "  Min: -296.3301\n",
      "  Max: 323.7922\n",
      "\n",
      "Layer 13:\n",
      "  Mean: -0.0049\n",
      "  Std Dev: 2.0862\n",
      "  Min: -296.4678\n",
      "  Max: 323.9347\n",
      "\n",
      "Layer 14:\n",
      "  Mean: -0.0053\n",
      "  Std Dev: 2.0872\n",
      "  Min: -296.5641\n",
      "  Max: 324.0419\n",
      "\n",
      "Layer 15:\n",
      "  Mean: -0.0048\n",
      "  Std Dev: 2.0889\n",
      "  Min: -296.6444\n",
      "  Max: 324.1428\n",
      "\n",
      "Layer 16:\n",
      "  Mean: -0.0055\n",
      "  Std Dev: 2.0902\n",
      "  Min: -296.6812\n",
      "  Max: 324.2944\n",
      "\n",
      "Layer 17:\n",
      "  Mean: -0.0058\n",
      "  Std Dev: 2.0930\n",
      "  Min: -296.8413\n",
      "  Max: 324.3992\n",
      "\n",
      "Layer 18:\n",
      "  Mean: -0.0060\n",
      "  Std Dev: 2.0961\n",
      "  Min: -297.0353\n",
      "  Max: 324.7107\n",
      "\n",
      "Layer 19:\n",
      "  Mean: -0.0048\n",
      "  Std Dev: 2.0988\n",
      "  Min: -297.0883\n",
      "  Max: 324.7747\n",
      "\n",
      "Layer 20:\n",
      "  Mean: -0.0052\n",
      "  Std Dev: 2.1014\n",
      "  Min: -297.1292\n",
      "  Max: 324.7881\n",
      "\n",
      "Layer 21:\n",
      "  Mean: -0.0050\n",
      "  Std Dev: 2.1045\n",
      "  Min: -297.1374\n",
      "  Max: 324.7462\n",
      "\n",
      "Layer 22:\n",
      "  Mean: -0.0056\n",
      "  Std Dev: 2.1094\n",
      "  Min: -297.1481\n",
      "  Max: 324.7343\n",
      "\n",
      "Layer 23:\n",
      "  Mean: -0.0057\n",
      "  Std Dev: 2.1138\n",
      "  Min: -297.1642\n",
      "  Max: 324.7183\n",
      "\n",
      "Layer 24:\n",
      "  Mean: -0.0053\n",
      "  Std Dev: 2.1197\n",
      "  Min: -297.1745\n",
      "  Max: 324.7222\n",
      "\n",
      "Layer 25:\n",
      "  Mean: -0.0072\n",
      "  Std Dev: 2.1249\n",
      "  Min: -297.1701\n",
      "  Max: 324.6898\n",
      "\n",
      "Layer 26:\n",
      "  Mean: -0.0087\n",
      "  Std Dev: 2.1314\n",
      "  Min: -297.0685\n",
      "  Max: 324.6751\n",
      "\n",
      "Layer 27:\n",
      "  Mean: -0.0092\n",
      "  Std Dev: 2.1408\n",
      "  Min: -297.0596\n",
      "  Max: 324.7065\n",
      "\n",
      "Layer 28:\n",
      "  Mean: -0.0106\n",
      "  Std Dev: 2.1503\n",
      "  Min: -296.7934\n",
      "  Max: 324.5114\n",
      "\n",
      "Layer 29:\n",
      "  Mean: -0.0083\n",
      "  Std Dev: 2.1628\n",
      "  Min: -296.6996\n",
      "  Max: 324.4012\n",
      "\n",
      "Layer 30:\n",
      "  Mean: -0.0105\n",
      "  Std Dev: 2.1837\n",
      "  Min: -296.3044\n",
      "  Max: 324.2018\n",
      "\n",
      "Layer 31:\n",
      "  Mean: -0.0123\n",
      "  Std Dev: 2.1899\n",
      "  Min: -294.1466\n",
      "  Max: 321.4954\n",
      "\n",
      "Layer 32:\n",
      "  Mean: -0.0007\n",
      "  Std Dev: 1.0558\n",
      "  Min: -21.9441\n",
      "  Max: 22.8590\n",
      "\n",
      "Layer analysis completed.\n"
     ]
    }
   ],
   "source": [
    "def register_hooks(model):\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Check if output is a tuple and extract the first element if it is\n",
    "        if isinstance(output, tuple):\n",
    "            activations.append(output[0])\n",
    "        else:\n",
    "            activations.append(output)\n",
    "\n",
    "    hooks = []\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        hooks.append(layer.register_forward_hook(hook_fn))\n",
    "    \n",
    "    return hooks, activations\n",
    "\n",
    "def analyze_layers(input_text):\n",
    "    # Tokenize the input text and move it to the appropriate device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Register hooks to capture the activations\n",
    "    hooks, activations = register_hooks(model)\n",
    "\n",
    "    # Run the model to trigger the hooks\n",
    "    _ = model(**inputs)\n",
    "    \n",
    "    # Now activations list will have outputs from each layer\n",
    "    print(f\"Analyzing LLaMA 3.1 8B model layers for input: '{input_text}'\\n\")\n",
    "    for i, activation in enumerate(activations):\n",
    "        # Compute statistics for each layer's output\n",
    "        mean_val = activation.mean().item()\n",
    "        std_val = activation.std().item()\n",
    "        min_val = activation.min().item()\n",
    "        max_val = activation.max().item()\n",
    "        \n",
    "        # Print the statistics for this layer\n",
    "        print(f\"Layer {i + 1}:\")\n",
    "        print(f\"  Mean: {mean_val:.4f}\")\n",
    "        print(f\"  Std Dev: {std_val:.4f}\")\n",
    "        print(f\"  Min: {min_val:.4f}\")\n",
    "        print(f\"  Max: {max_val:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Remove hooks after analysis\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    print(\"Layer analysis completed.\")\n",
    "\n",
    "# Example input text for analysis\n",
    "starting_input = \"In the distant future, humanity has mastered the art of interstellar travel.\"\n",
    "\n",
    "analyze_layers(starting_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise Analysis of LLaMA 3.1 8B Model\n",
    "\n",
    "This section provides a detailed analysis of the activations for each layer of the LLaMA 3.1 8B model when processing the input text:\n",
    "\n",
    "**Input Text:** \"In the distant future, humanity has mastered the art of interstellar travel.\"\n",
    "\n",
    "#### General Structure:\n",
    "- **Layer X:** Refers to the Xth layer of the model.\n",
    "  - **Mean:** The average value of all activations in the layer.\n",
    "  - **Std Dev:** The standard deviation, representing the spread or variability of the activations.\n",
    "  - **Min:** The minimum activation value in the layer.\n",
    "  - **Max:** The maximum activation value in the layer.\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "1. **Early Layers (Layer 1):**\n",
    "   - **Mean:** 0.0002\n",
    "   - **Std Dev:** 0.0491\n",
    "   - **Min:** -2.5502\n",
    "   - **Max:** 3.2153\n",
    "   - **Interpretation:** Early layers have tightly clustered activations around zero with small variability, focusing on basic feature extraction.\n",
    "\n",
    "2. **Middle Layers (Layer 2 - Layer 31):**\n",
    "   - **Mean:** -0.0052 (Layer 2) to -0.0123 (Layer 31)\n",
    "   - **Std Dev:** Ranges from 2.0646 to 2.1899\n",
    "   - **Min/Max:** Values range from -297.1374 to 324.7462 across these layers.\n",
    "   - **Interpretation:** These layers exhibit high variability and wide activation ranges, indicating significant transformations and complex feature extraction.\n",
    "\n",
    "3. **Final Layer (Layer 32):**\n",
    "   - **Mean:** -0.0007\n",
    "   - **Std Dev:** 1.0558\n",
    "   - **Min:** -21.9441\n",
    "   - **Max:** 22.8590\n",
    "   - **Interpretation:** The final layer shows reduced variability and a narrower range, indicating that the model is consolidating the information into a more stable output.\n",
    "\n",
    "#### Summary:\n",
    "- **Activation Variability:** Increasing standard deviation in middle layers suggests intensive processing and abstraction.\n",
    "- **Activation Range:** Extreme values in middle layers reflect the model's complex transformations, while the final layer shows a consolidation of features.\n",
    "- **Final Layer:** Reduced variability and range, preparing for a cohesive and interpretable output.\n",
    "\n",
    "This analysis helps to understand the progression of information processing within the LLaMA 3.1 8B model, highlighting the role of each layer in transforming the input text into a final representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The capital of India is Delhi, what is the capital of France? It is'\n",
      "Ranked candidates:\n",
      "  ' Berlin': -32.8896\n",
      "  'Paris': -33.6316\n",
      "  ' London': -33.7240\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def rank_candidates(input_text, candidates):\n",
    "    # Device management\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "    \n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    logprobs = {}\n",
    "\n",
    "    for candidate in candidates:\n",
    "        # Tokenize candidate and combine with input text\n",
    "        candidate_ids = tokenizer(candidate, return_tensors=\"pt\").input_ids.to(device)\n",
    "        combined_ids = torch.cat((input_ids, candidate_ids), dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get logits for all tokens in the candidate in a single forward pass\n",
    "            output = model(input_ids=combined_ids)\n",
    "            logits = output.logits[:, input_ids.size(1)-1:-1, :]\n",
    "\n",
    "        # Compute log probabilities for each token in the candidate\n",
    "        logprobs_step = F.log_softmax(logits, dim=-1)\n",
    "        candidate_logprobs = logprobs_step.gather(2, candidate_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Sum log probabilities to get the total log probability for the candidate\n",
    "        total_logprob = candidate_logprobs.sum().item()\n",
    "        \n",
    "        logprobs[candidate] = total_logprob\n",
    "\n",
    "    # Sort candidates by their log probability in descending order\n",
    "    sorted_candidates = sorted(logprobs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Output the ranked candidates\n",
    "    print(f\"Input: '{input_text}'\")\n",
    "    print(\"Ranked candidates:\")\n",
    "    for candidate, logprob in sorted_candidates:\n",
    "        print(f\"  '{candidate}': {logprob:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "rank_candidates(\"The capital of India is Delhi, what is the capital of France? It is\", [\"Paris\", \" Berlin\", \" London\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'}]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", max_new_tokens=50, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\"))\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate method default parameters:\n",
      "(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "# Access the underlying model and tokenizer\n",
    "model = pipe.model\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Inspect the generate method's signature to see default values\n",
    "print(\"Generate method default parameters:\")\n",
    "print(inspect.signature(model.generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration parameters:\n",
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the model's default configuration\n",
    "print(\"Model configuration parameters:\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input: {'input_ids': tensor([[128000,  15546,    527,    499,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Access the tokenizer and model\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Prepare the input message\n",
    "messages = [{\"role\": \"user\", \"content\": \"Who are you?\"}]\n",
    "\n",
    "# You might need to convert the input to a format similar to what the pipeline does\n",
    "input_text = messages[0][\"content\"]\n",
    "\n",
    "# Tokenize the input\n",
    "tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(\"Tokenized input:\", tokenized_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who are you?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_input['input_ids'].squeeze().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you? Who is your family?\n",
      "I am a 5-year-old, 10-month-old, and a 6-month-old child. My family is a loving and supportive one, and we are all excited to start this new chapter in our lives. My\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Prepare the input message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Convert the input message to a single string (assuming one message)\n",
    "input_text = messages[0][\"content\"]\n",
    "\n",
    "# Tokenize the input text and create attention mask\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Move tensors to the appropriate device (CUDA or MPS or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate the output with attention mask\n",
    "output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the generated token IDs to text\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
