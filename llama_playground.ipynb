{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from hf_token import hf_token\n",
    "\n",
    "login(hf_token)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadityabhat/dev/llama_playground/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [03:05<00:00, 46.46s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[128000,   9906,     11,   1268,    527,    499,     30]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
      "Logits shape: torch.Size([1, 7, 128256])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the input IDs and attention mask\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Check the shape of the logits (batch size, sequence length, vocabulary size)\n",
    "print(\"Logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n",
      " LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "model LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens Embedding(128256, 4096)\n",
      "model.layers ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "model.layers.0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.0.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.1.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.2.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.3.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.4.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.5.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.6.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.7.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.8.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.9.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.10.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.11.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.12.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.13.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.14.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.15.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.16.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.17.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.18.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.19.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.20.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.21.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.22.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.23.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.24.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.25.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.26.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.27.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.28.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLU()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.29.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLU()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.30.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLU()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.31.self_attn LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb LlamaRotaryEmbedding()\n",
      "model.layers.31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLU()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.norm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.rotary_emb LlamaRotaryEmbedding()\n",
      "lm_head Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all named parameters to see their names and types\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "# You can also use named_modules to explore the submodules\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.0166, -0.0062, -0.0014,  ...,  0.0149, -0.0128, -0.0027],\n",
      "        [-0.0004, -0.0276, -0.0031,  ...,  0.0132,  0.0059, -0.0082],\n",
      "        [ 0.0131,  0.0008,  0.0139,  ..., -0.0016,  0.0061,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0155, -0.0123,  ..., -0.0120,  0.0111, -0.0161],\n",
      "        [-0.0045, -0.0013, -0.0025,  ..., -0.0195, -0.0192,  0.0029],\n",
      "        [-0.0027,  0.0103,  0.0063,  ..., -0.0065,  0.0166, -0.0077]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model to understand its structure\n",
    "print(model)\n",
    "\n",
    "# Access the first transformer block\n",
    "first_block = model.model.layers[0]\n",
    "\n",
    "# Print the first linear layer weights of the first transformer block\n",
    "first_linear_layer_weights = first_block.mlp.gate_proj.weight\n",
    "print(first_linear_layer_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the attention weights from the first attention block\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attentions = outputs.attentions  # Tuple of attention matrices\n",
    "\n",
    "# Visualize the attention of the first head in the first layer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(attentions[0][0][0].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word:  Paris\n"
     ]
    }
   ],
   "source": [
    "# Different input text\n",
    "input_text = \"What is the capital of France?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Decode the model's output\n",
    "predicted_token_id = torch.argmax(logits[0, -1, :])\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(\"Predicted word:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: What is the capital of France? Paris\n",
      "What is the capital of Australia? Canberra\n",
      "What is the capital of South Africa? Pretoria\n",
      "What is the capital of Canada? Ottawa\n",
      "What is the capital of Egypt? Cairo\n",
      "What is\n"
     ]
    }
   ],
   "source": [
    "# Generate text from the model\n",
    "generated_text = model.generate(inputs['input_ids'], max_length=50)\n",
    "print(\"Generated Text:\", tokenizer.decode(generated_text[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside LlamaSdpaAttention\n",
      "Input: ()\n",
      "Output: (tensor([[[ 2.1353e-03, -1.7818e-03, -1.6321e-04,  ...,  2.6612e-03,\n",
      "          -4.9833e-04, -1.1525e-03],\n",
      "         [ 2.4133e-03, -1.9542e-05, -8.1634e-04,  ...,  2.4769e-03,\n",
      "          -3.3559e-04, -7.2372e-04],\n",
      "         [ 2.8779e-03,  3.1731e-03,  4.9005e-03,  ...,  8.1639e-03,\n",
      "          -3.7740e-04, -4.8833e-03],\n",
      "         ...,\n",
      "         [-5.5615e-03,  6.9881e-03, -2.0973e-03,  ...,  6.0605e-03,\n",
      "          -3.6149e-06, -5.4601e-03],\n",
      "         [-8.4292e-03,  1.3621e-02,  1.0348e-03,  ...,  1.0263e-02,\n",
      "           1.2356e-02,  5.1786e-03],\n",
      "         [ 3.1191e-03,  6.6197e-03, -5.0190e-03,  ...,  8.8916e-03,\n",
      "           4.0729e-03, -2.6361e-03]]], grad_fn=<UnsafeViewBackward0>), None, DynamicCache())\n"
     ]
    }
   ],
   "source": [
    "# Define the hook function\n",
    "def hook_fn(module, input, output):\n",
    "    print(f\"Inside {module.__class__.__name__}\")\n",
    "    print(f\"Input: {input}\")\n",
    "    print(f\"Output: {output}\")\n",
    "\n",
    "# Access the first attention layer in the first transformer block\n",
    "first_attention_layer = model.model.layers[0].self_attn\n",
    "\n",
    "# Register the forward hook\n",
    "handle = first_attention_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Prepare the inputs\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass to trigger the hook\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Remove the hook after use\n",
    "handle.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: mps:0\n",
      "Input tensor is on device: mps:0\n",
      "Output tensor is on device: mps:0\n",
      "Generated text: The capital of France is a city of romance and beauty, with a rich history that spans thousands of years. From the iconic Eiffel Tower to the world-class museums and art galleries, Paris is a destination that has something for everyone.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Check if Metal (MPS) is available and set the device accordingly\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the device of the model\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Tokenize the input text (remains on CPU)\n",
    "input_text = \"The capital of France is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the MPS device (if using MPS)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Print the device of the input tensor\n",
    "print(f\"Input tensor is on device: {inputs['input_ids'].device}\")\n",
    "\n",
    "# Generate text using the model on GPU\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "# Print the device of the output tensor before moving to CPU\n",
    "print(f\"Output tensor is on device: {outputs.device}\")\n",
    "\n",
    "# Move the outputs back to CPU before decoding\n",
    "outputs = outputs.cpu()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability for ' Paris.': -38.761497497558594\n",
      "Log probability for ' Berlin.': -36.26842212677002\n",
      "Log probability for ' London.': -39.63212966918945\n"
     ]
    }
   ],
   "source": [
    "# Input and candidate outputs\n",
    "input_text = \"The capital of France is\"\n",
    "candidate_outputs = [\" Paris.\", \" Berlin.\", \" London.\"]\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Store log probabilities for each candidate\n",
    "logprobs = {}\n",
    "\n",
    "for candidate in candidate_outputs:\n",
    "    # Tokenize the candidate text\n",
    "    candidate_ids = tokenizer(candidate, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Concatenate input and candidate tokens\n",
    "    combined_ids = torch.cat((input_ids, candidate_ids), dim=-1)\n",
    "\n",
    "    # Initialize log probability\n",
    "    total_logprob = 0.0\n",
    "\n",
    "    # Loop over each token in the candidate sequence\n",
    "    for i in range(candidate_ids.size(1)):\n",
    "        # Get logits for the current step\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=combined_ids[:, :input_ids.size(1) + i + 1])\n",
    "            logits = output.logits[:, -1, :]  # Logits of the last token\n",
    "        \n",
    "        # Convert logits to log probabilities\n",
    "        logprobs_step = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get the log probability of the actual next token\n",
    "        token_logprob = logprobs_step[:, candidate_ids[0, i]].item()\n",
    "        \n",
    "        # Add to the total log probability\n",
    "        total_logprob += token_logprob\n",
    "    \n",
    "    # Store the total log probability for the candidate\n",
    "    logprobs[candidate] = total_logprob\n",
    "\n",
    "# Print the logprobs\n",
    "for candidate, logprob in logprobs.items():\n",
    "    print(f\"Log probability for '{candidate}': {logprob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word:  a\n"
     ]
    }
   ],
   "source": [
    "# Different input text\n",
    "input_text = \"The capital of France is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Decode the model's output\n",
    "predicted_token_id = torch.argmax(logits[0, -1, :])\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(\"Predicted word:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: <|begin_of_text|>The capital of France is Paris, and the official language is French. The country is divided into 13 regions, each with its own unique culture and history. France is known for its rich history, art, fashion, cuisine, and wine.\n",
      "France is a popular tourist destination, attracting millions of visitors each year. The Eiffel Tower, the Louvre Museum, and the Palace of Versailles are just a few of the many iconic landmarks that draw visitors to the country. France is also\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text (remains on CPU)\n",
    "input_text = \"The capital of France is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the MPS device (if using MPS)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Explicitly set the attention mask and pad token ID\n",
    "attention_mask = inputs['attention_mask']\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "# Generate text\n",
    "output_sequences = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=attention_mask,  # Pass the attention mask\n",
    "    max_length=100,  # Adjust the max_length as needed\n",
    "    do_sample=True,  # Set to True if you want stochastic sampling (e.g., for creative text generation)\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    "    pad_token_id=pad_token_id,  # Explicitly set the pad token ID\n",
    "    eos_token_id=tokenizer.eos_token_id,  # Stop at the end-of-sequence token\n",
    ")\n",
    "\n",
    "# Move outputs back to CPU before decoding (if needed)\n",
    "output_sequences = output_sequences.cpu()\n",
    "\n",
    "# Decode the generated sequences\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (Metal) is available and being used.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    print(\"GPU (Metal) is available and being used.\")\n",
    "else:\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
